{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#THIS IS THE ASSIGNMENT ATTEMPT STEPS:\n",
    "> ALL THE EXPERIMENTS ARE DONE FOR DATASET-1 requests_opened_external (due to time constraints)\n",
    "> From Hereon Dataset-1: requests_opened_external and Dataset-2: requests_closed_external\n",
    "> 1) **The Cleaning and Reframing Dataset-1**: The Dataset-1 contained multiple entries of Volume corresponding to a single date for different time, so aggregated the volumes across times corresponding to same date after normalizing the Date, Time, Volume -> date, time, volume. Even the date format was mm/dd/yyyy different from yyyy-mm-dd of dataset-2, so brought the date format in that format. Basically, brought dataset-1 to dataset-2(the reference dataset) format. Also observed that there were some days missing, and by comparing to calendar, it seemed that most of those missing days were weekends and some were the weekdays(indicating the voluntary or business holiday). But they were not used in fine-tuning the LLMs. The reason comes in Feature Modelling Section\n",
    "\n",
    "> 2) **Feature Engineering and Modelling**:\n",
    "> a)Different features like lag, rolling window, ema, calendar feature(including gap_day feature indicating the holidays), Fourier Terms(for month, and year), and standardized the volume history. z = log_10(1 + y_target), where y_target = next_day_volume. So the target variable is z now.!\n",
    "> b) Selected top 16 Features by Finding Coorelation between features and the target variable, using Pearson Coefficients -> Feature_Group_1\n",
    "> c) Selected top 16 Features by using XGBoost technique -> Feature_Group_2\n",
    "\n",
    "> 3) **Model Architecture and Training**: LoRA was used for the efficient training. which reduced the trainable parameter to almost 0.5% for TinyLlama case, and 1.13% for Qwen 2.5 7 B, model of their total trainable parameters. Nvidia A100, 40 GB was used.!\n",
    " a)Used TinyLlaMa 1.1 B chat, and Qwen 2.5 7B for fine-tuning. b) before moving to Qwen 2.5 7B, some 3 experiments were perfomed on TinyLlaMa 1.1 B, which are : i) Fine-tuning using Feature_Group_1, with 3 epochs ii) Training of Numerical Value head after Training TinyLlama on Feature_Group_1(with 3 epochs), with 5 epochs iii) Training Tiny Llama on Feature_Group_2, with 5 epochs. c) Trained Qwen 2.5 7B, with Feature_Group_1, 3 epochs.\n",
    "\n",
    "> 4) **Model Evaluation**: The Metrics like RMSE, MSE, MAE and MAPE were used to check the performance of the trained Models. These values were compared with the Naive Method, like seasonal_naive(with lag = 5 days), moving average(with window = 7 days). it turned out that Tiny Llama, trained on Feature_Group_2 with 5 epochs was the best, and the Qwen2.5 7B trained on Feature_Group_1, 3 epochs was very near to the performance of the former one. Inferen\n",
    ">5) **Inference**:  It's my hypothesis that, if the Qwen is trained on Feature_Group_2 or Feature_Group_1 with 5 epochs, can outperform TinyLlama easily.\n",
    "\n",
    ">6) **Production**: Both the models can be exported for production, after including the LoRA adapters,(as in one case when I did not used the LoRA adapters, my predictions blew up to inf.).\n",
    "\n",
    "\n",
    "**PS**:\n",
    "> a) Due to bitsandbytes dependency problem, 4 bit/8bit quantization is not done, and current setup runs on bf16 precision\n",
    ">b) I by chance, overwrite the training of TinyLlama on Feature_Group_1, 3 epochs with TinyLlama on Feature_Group_2, 5 epochs. Which is attached in Different notebook.\n",
    ">c) The selection of Top K features by Pearson Coefficient occurs in the Training code only!\n",
    "\n",
    "*Please Forgive Any Inconvenience Caused*."
   ],
   "metadata": {
    "id": "6gYaXVAFDHGk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cleaning of the Dataset-1(requests_opened_external.csv)"
   ],
   "metadata": {
    "id": "pSDKrSBmjrEJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==== CONFIG ====\n",
    "PATH_IN  = \"/content/requests_opened_external.csv\"\n",
    "PATH_OUT = \"/content/cleaned_requests_opened_daily.csv\"\n",
    "\n",
    "# ==== LOAD ====\n",
    "df = pd.read_csv(PATH_IN)\n",
    "\n",
    "# 1) normalize column titles to lower case\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "date_col, volume_col = \"date\", \"volume\"\n",
    "\n",
    "# 2) parse mm/dd/yyyy -> yyyy-mm-dd\n",
    "df[date_col] = pd.to_datetime(df[date_col], format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "bad_dates = df[date_col].isna().sum()\n",
    "if bad_dates:\n",
    "    print(f\"[WARN] Dropping {bad_dates} rows with invalid dates.\")\n",
    "df = df.dropna(subset=[date_col])\n",
    "df[\"date\"] = df[date_col].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Ensure volume is numeric\n",
    "df[volume_col] = pd.to_numeric(df[volume_col], errors=\"coerce\")\n",
    "nan_vols = df[volume_col].isna().sum()\n",
    "if nan_vols:\n",
    "    print(f\"[WARN] {nan_vols} rows have non-numeric '{volume_col}'. Treating as 0 for checks.\")\n",
    "vol_before = df[volume_col].fillna(0)\n",
    "\n",
    "# 4) SANITY CHECK BEFORE AGG\n",
    "non_integer_before = int(((vol_before.astype(float) % 1) != 0).sum())\n",
    "dtype_before = df[volume_col].dtype\n",
    "print(f\"Before aggregation → dtype={dtype_before}, non-integer rows={non_integer_before}\")\n",
    "\n",
    "# 3) AGGREGATE: sum cross time per date\n",
    "daily = df.groupby(\"date\", as_index=False)[volume_col].sum(min_count=1)\n",
    "\n",
    "# ---- POST-AGG CHECK ----\n",
    "\n",
    "daily_vol = pd.to_numeric(daily[volume_col], errors=\"coerce\").fillna(0).astype(float)\n",
    "non_integer_after = int(((daily_vol % 1) != 0).sum())\n",
    "dtype_after = daily[volume_col].dtype\n",
    "print(f\"After aggregation  → dtype={dtype_after}, non-integer daily rows={non_integer_after}\")\n",
    "\n",
    "# Enforce integer output (round as a guard, then cast)\n",
    "daily[\"volume\"] = daily_vol.round().astype(\"int64\")\n",
    "daily = daily.sort_values(\"date\")\n",
    "\n",
    "# SAVE\n",
    "daily.to_csv(PATH_OUT, index=False)\n",
    "print(f\"Saved daily aggregates to: {PATH_OUT}\")\n",
    "print(daily.head(10))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYcZw_1pnECQ",
    "outputId": "8c69b62a-924e-4d99-a50c-37d1a8d580e2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before aggregation → dtype=int64, non-integer rows=0\n",
      "After aggregation  → dtype=int64, non-integer daily rows=0\n",
      "Saved daily aggregates to: /content/cleaned_requests_opened_daily.csv\n",
      "         date    volume\n",
      "0  1998-01-02  10139741\n",
      "1  1998-01-05  19720368\n",
      "2  1998-01-06  14116033\n",
      "3  1998-01-07  16859831\n",
      "4  1998-01-08  15860442\n",
      "5  1998-01-09  28586578\n",
      "6  1998-01-12  24067349\n",
      "7  1998-01-13  13279676\n",
      "8  1998-01-14  16616568\n",
      "9  1998-01-15  24826372\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Feature Engineering and Modelling"
   ],
   "metadata": {
    "id": "zKwTkGbAkKRT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7Hocb2mkEn4",
    "outputId": "64c096d5-1332-46a6-f268-81a5b71dd898"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved features: /content/features_trading_only.csv\n",
      "Saved scaler  : /content/features_trading_only_scaler.json\n",
      "        date      volume   y_trading    y_log1p  z_target\n",
      "0 1998-02-11  15819189.0  13524081.0  16.419983  0.920051\n",
      "1 1998-02-12  13524081.0   8694402.0  15.978190  0.337016\n",
      "2 1998-02-13   8694402.0  14912102.0  16.517684  1.048988\n",
      "3 1998-02-17  14912102.0  12788824.0  16.364082  0.846279\n",
      "4 1998-02-18  12788824.0  16986901.0  16.647953  1.220905 \n",
      "\n",
      "Rows: 6890 | Cols: 53\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "INPUT_CSV   = \"/content/cleaned_requests_opened_daily.csv\"\n",
    "OUTPUT_CSV  = \"/content/features_trading_only.csv\"\n",
    "SCALER_JSON = \"/content/features_trading_only_scaler.json\"\n",
    "\n",
    "DATE_COL = \"date\"\n",
    "VOL_COL  = \"volume\"\n",
    "\n",
    "ROLL_WINDOWS = [7, 14, 28]\n",
    "EMA_SPANS    = [7, 28]\n",
    "FOURIER_K    = 3\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "if DATE_COL not in df.columns or VOL_COL not in df.columns:\n",
    "    raise ValueError(f\"Input must contain columns: '{DATE_COL}', '{VOL_COL}'\")\n",
    "\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[DATE_COL]).sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "\n",
    "df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "df[VOL_COL] = df[VOL_COL].astype(float)\n",
    "\n",
    "# CALENDAR FEATURES\n",
    "df[\"dow\"]    = df[DATE_COL].dt.weekday\n",
    "df[\"dom\"]    = df[DATE_COL].dt.day\n",
    "df[\"week\"]   = df[DATE_COL].dt.isocalendar().week.astype(int)\n",
    "df[\"month\"]  = df[DATE_COL].dt.month\n",
    "df[\"quarter\"]= df[DATE_COL].dt.quarter\n",
    "df[\"year\"]   = df[DATE_COL].dt.year\n",
    "\n",
    "df[\"is_month_start\"]   = df[DATE_COL].dt.is_month_start.astype(int)\n",
    "df[\"is_month_end\"]     = df[DATE_COL].dt.is_month_end.astype(int)\n",
    "df[\"is_quarter_start\"] = df[DATE_COL].dt.is_quarter_start.astype(int)\n",
    "df[\"is_quarter_end\"]   = df[DATE_COL].dt.is_quarter_end.astype(int)\n",
    "df[\"is_year_start\"]    = df[DATE_COL].dt.is_year_start.astype(int)\n",
    "df[\"is_year_end\"]      = df[DATE_COL].dt.is_year_end.astype(int)\n",
    "\n",
    "# gaps between trading days\n",
    "df[\"gap_days_prev\"] = (df[DATE_COL] - df[DATE_COL].shift(1)).dt.days\n",
    "df[\"gap_days_next\"] = (df[DATE_COL].shift(-1) - df[DATE_COL]).dt.days\n",
    "\n",
    "# ROLLING / EMA / LAGS / MOMENTUM\n",
    "v = df[VOL_COL]\n",
    "\n",
    "for w in ROLL_WINDOWS:\n",
    "    df[f\"roll{w}_mean\"] = v.rolling(w).mean()\n",
    "    df[f\"roll{w}_sum\"]  = v.rolling(w).sum()\n",
    "    df[f\"roll{w}_std\"]  = v.rolling(w).std()\n",
    "    df[f\"roll{w}_min\"]  = v.rolling(w).min()\n",
    "    df[f\"roll{w}_max\"]  = v.rolling(w).max()\n",
    "\n",
    "for span in EMA_SPANS:\n",
    "    df[f\"ema{span}\"] = v.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "df[\"lag1\"] = v.shift(1)\n",
    "df[\"lag2\"] = v.shift(2)\n",
    "df[\"lag5\"] = v.shift(5)\n",
    "\n",
    "df[\"ret1\"] = (v - v.shift(1)) / (v.shift(1).abs() + 1.0)\n",
    "df[\"ret7\"] = (v - v.shift(7)) / (v.shift(7).abs() + 1.0)\n",
    "\n",
    "# FOURIER FEATURES\n",
    "# (A) Trading-index seasonality\n",
    "t = np.arange(len(df))\n",
    "for k in range(1, FOURIER_K+1):\n",
    "    df[f\"fourier_t_sin{k}\"] = np.sin(2*np.pi*k*t/len(df))\n",
    "    df[f\"fourier_t_cos{k}\"] = np.cos(2*np.pi*k*t/len(df))\n",
    "\n",
    "# (B) Annual seasonality by year-fraction\n",
    "day_of_year = df[DATE_COL].dt.dayofyear.astype(float)\n",
    "year_len = df[DATE_COL].dt.is_leap_year.map({True:366, False:365}).astype(float)\n",
    "yf = day_of_year / year_len\n",
    "for k in range(1, FOURIER_K+1):\n",
    "    df[f\"fourier_y_sin{k}\"] = np.sin(2*np.pi*k*yf)\n",
    "    df[f\"fourier_y_cos{k}\"] = np.cos(2*np.pi*k*yf)\n",
    "\n",
    "# TARGETS\n",
    "# Next trading day volume\n",
    "df[\"y_trading\"] = df[VOL_COL].shift(-1)\n",
    "\n",
    "# z_target\n",
    "df[\"y_log1p\"] = np.log1p(df[\"y_trading\"])\n",
    "\n",
    "# drop last row\n",
    "df = df.dropna(subset=[\"y_trading\", \"y_log1p\"]).reset_index(drop=True)\n",
    "\n",
    "mu = float(df[\"y_log1p\"].mean())\n",
    "sigma = float(df[\"y_log1p\"].std(ddof=0)) if df[\"y_log1p\"].std(ddof=0) > 0 else 1.0\n",
    "df[\"z_target\"] = (df[\"y_log1p\"] - mu) / sigma\n",
    "\n",
    "#FINAL CLEANUP\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "df[num_cols] = df[num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# SAVE\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "with open(SCALER_JSON, \"w\") as f:\n",
    "    json.dump({\"y_log1p_mean\": mu, \"y_log1p_std\": sigma, \"note\": \"y = expm1(z*sigma + mu)\"}, f, indent=2)\n",
    "\n",
    "print(f\"Saved features: {OUTPUT_CSV}\")\n",
    "print(f\"Saved scaler  : {SCALER_JSON}\")\n",
    "print(df[[DATE_COL, VOL_COL, \"y_trading\", \"y_log1p\", \"z_target\"]].head(), \"\\n\")\n",
    "print(\"Rows:\", len(df), \"| Cols:\", len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model Architecture and Training"
   ],
   "metadata": {
    "id": "rWbrU0Y6PSyq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "!pip -q install -U \"transformers>=4.46.1\" \"datasets==2.20.0\" \"accelerate>=0.34.0\" \\\n",
    "                      \"peft==0.13.0\" \"scikit-learn\" \"einops\" \"evaluate\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5t3N3M5tRQQ",
    "outputId": "4c7fa79b-b611-4053-ff32-d34c32c5900a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/547.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/322.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/9.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/9.5 MB\u001b[0m \u001b[31m162.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m187.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOS2R49vvCb1",
    "outputId": "949c0477-ba7f-4659-81e6-dbbfb4ddb192"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.55.4\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Model Training and Evaluation"
   ],
   "metadata": {
    "id": "1wrKbLTi19pM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TinyLlama 1.1B parameters, training with top-16 features selected by XGBoost, 5 epochs"
   ],
   "metadata": {
    "id": "31fEzGoT2G77"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import os, re, math, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import set_seed\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#  CONFIG\n",
    "CONFIG = {\n",
    "    \"feature_files\": [\"/content/features_trading_only_2.csv\"],\n",
    "    \"date_col\": \"date\",\n",
    "    \"vol_col\": \"volume\",\n",
    "    \"label_col\": \"z_target\",\n",
    "    \"context_len\": 64,\n",
    "    \"max_features\": 16,\n",
    "    \"model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"seed\": 42,\n",
    "    \"train_frac\": 0.9,\n",
    "    \"epochs\": 5,\n",
    "    \"lr\": 1e-4,\n",
    "    \"train_bs\": 2,\n",
    "    \"grad_accum\": 16,\n",
    "    \"max_length\": 1024,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"output_dir\": \"/content/tinyllama_ts_lora\",\n",
    "    \"bf16\": True,\n",
    "}\n",
    "\n",
    "set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "# LOAD & MERGE FEATURES\n",
    "def load_and_merge(paths: List[str], date_col: str):\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        df = pd.read_csv(p)\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[date_col])\n",
    "        dfs.append(df)\n",
    "    all_df = pd.concat(dfs, axis=0, ignore_index=True).sort_values(date_col).reset_index(drop=True)\n",
    "    all_df = all_df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "    return all_df\n",
    "\n",
    "df = load_and_merge(CONFIG[\"feature_files\"], CONFIG[\"date_col\"])\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(df[[CONFIG[\"date_col\"], CONFIG[\"vol_col\"], \"y_trading\", \"y_log1p\", CONFIG[\"label_col\"]]].head())\n",
    "\n",
    "# SELECTING TOP-K FEATURES\n",
    "EXCLUDE_COLS = {CONFIG[\"date_col\"], CONFIG[\"label_col\"], CONFIG[\"vol_col\"], \"y_trading\", \"y_log1p\"}\n",
    "num = df.select_dtypes(include=[np.number]).copy()\n",
    "std = num.std(numeric_only=True)\n",
    "non_const = std[std > 0].index.tolist()\n",
    "num = num[non_const]\n",
    "cand = [c for c in num.columns if c not in EXCLUDE_COLS]\n",
    "if not cand:\n",
    "    raise ValueError(\"No candidate numeric features found after exclusions.\")\n",
    "corr = num[cand].corrwith(num[CONFIG[\"label_col\"]]).abs().replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "feature_cols = corr.sort_values(ascending=False).index.tolist()[:CONFIG[\"max_features\"]]\n",
    "print(\"Selected feature columns:\", feature_cols)\n",
    "\n",
    "# BUILD z_history SERIES\n",
    "# History from standardized log1p(volume) to stabilize scale\n",
    "vol = df[CONFIG[\"vol_col\"]].astype(float).values.reshape(-1,1)\n",
    "z_hist_scaler = StandardScaler()\n",
    "z_hist_series = z_hist_scaler.fit_transform(np.log1p(vol)).reshape(-1)\n",
    "\n",
    "#  BUILD WINDOWS\n",
    "def make_windows(df: pd.DataFrame, z_hist: np.ndarray, ctx: int, feat_cols: List[str], label_col: str):\n",
    "    X, Y = [], []\n",
    "    n = len(df)\n",
    "    for t in range(ctx, n):\n",
    "        hist = z_hist[t-ctx:t].tolist()\n",
    "        feats = df.iloc[t][feat_cols].to_dict()\n",
    "        y = float(df.iloc[t][label_col])\n",
    "        X.append((hist, feats))\n",
    "        Y.append(y)\n",
    "    return X, np.array(Y, dtype=np.float32)\n",
    "\n",
    "X_raw, Y = make_windows(df, z_hist_series, CONFIG[\"context_len\"], feature_cols, CONFIG[\"label_col\"])\n",
    "print(\"Total samples:\", len(X_raw))\n",
    "\n",
    "# TRAIN / VAL SPLIT\n",
    "N = len(X_raw)\n",
    "cut = int(N * CONFIG[\"train_frac\"])\n",
    "train_idx = np.arange(0, cut)\n",
    "val_idx = np.arange(cut, N)\n",
    "\n",
    "def ex_to_text(hist, feats, y_z):\n",
    "    hist_str = \", \".join(f\"{x:.4f}\" for x in hist)\n",
    "    feats_str = \", \".join(f\"{k}={float(v):.4f}\" for k,v in feats.items()) if feats else \"none\"\n",
    "    prompt = f\"z_hist[{len(hist)}]:{hist_str}\\nfeat:{feats_str}\\nnext_z:\"\n",
    "    target = f\"{y_z:.5f}\\n\"\n",
    "    return {\"prompt\": prompt, \"target\": target}\n",
    "\n",
    "train_text = [ex_to_text(*X_raw[i], Y[i]) for i in train_idx]\n",
    "val_text   = [ex_to_text(*X_raw[i], Y[i]) for i in val_idx]\n",
    "\n",
    "# TOKENIZER / MODEL\n",
    "model_name = CONFIG[\"model_name\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "torch_dtype = (\n",
    "    torch.bfloat16\n",
    "    if (CONFIG[\"bf16\"] and torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8)\n",
    "    else torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",   # no extra installs\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# LoRA targets for TinyLlama blocks\n",
    "lora_cfg = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"], lora_alpha=CONFIG[\"lora_alpha\"], lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# DATASET\n",
    "class TxtDS(Dataset):\n",
    "    def __init__(self, examples, tok, max_len=1024):\n",
    "        self.ex = examples; self.tok = tok; self.max_len = max_len\n",
    "    def __len__(self): return len(self.ex)\n",
    "    def __getitem__(self, i):\n",
    "        e = self.ex[i]\n",
    "        p_ids = self.tok(e[\"prompt\"], add_special_tokens=False)[\"input_ids\"]\n",
    "        t_ids = self.tok(e[\"target\"], add_special_tokens=False)[\"input_ids\"]\n",
    "        ids = p_ids + t_ids\n",
    "        if len(ids) > self.max_len:\n",
    "            overflow = len(ids) - self.max_len\n",
    "            keep_p = max(0, len(p_ids) - overflow)\n",
    "            ids = p_ids[-keep_p:] + t_ids\n",
    "        p_len = min(len(p_ids), len(ids) - len(t_ids))\n",
    "        labels = [-100]*p_len + ids[p_len:]\n",
    "        attn = [1]*len(ids)\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def pad_batch(batch, pad_id):\n",
    "    mx = max(len(b[\"input_ids\"]) for b in batch)\n",
    "    out = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    for b in batch:\n",
    "        pad_n = mx - len(b[\"input_ids\"])\n",
    "        out[\"input_ids\"].append(torch.cat([b[\"input_ids\"], torch.full((pad_n,), pad_id, dtype=torch.long)]))\n",
    "        out[\"attention_mask\"].append(torch.cat([b[\"attention_mask\"], torch.zeros(pad_n, dtype=torch.long)]))\n",
    "        out[\"labels\"].append(torch.cat([b[\"labels\"], torch.full((pad_n,), -100, dtype=torch.long)]))\n",
    "    return {k: torch.stack(v) for k,v in out.items()}\n",
    "\n",
    "def collate_fn(features):\n",
    "    return pad_batch(features, tokenizer.pad_token_id)\n",
    "\n",
    "train_ds = TxtDS(train_text, tokenizer, CONFIG[\"max_length\"])\n",
    "val_ds   = TxtDS(val_text, tokenizer, CONFIG[\"max_length\"])\n",
    "\n",
    "# TRAIN\n",
    "args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"train_bs\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"train_bs\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"grad_accum\"],\n",
    "    learning_rate=CONFIG[\"lr\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=(torch_dtype==torch.bfloat16),\n",
    "    fp16=(torch_dtype==torch.float16),\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=\"none\",\n",
    "    do_eval=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "#EVAL (invert to original units)\n",
    "\n",
    "def load_scaler_json(paths):\n",
    "    base = os.path.dirname(paths[0])\n",
    "    cand = os.path.join(base, \"features_trading_only_scaler_2.json\")\n",
    "    if os.path.exists(cand):\n",
    "        with open(cand, \"r\") as f:\n",
    "            s = json.load(f)\n",
    "        return float(s[\"y_log1p_mean\"]), float(s[\"y_log1p_std\"])\n",
    "    mu = float(df[\"y_log1p\"].mean()) if \"y_log1p\" in df.columns else 0.0\n",
    "    sigma = float(df[\"y_log1p\"].std(ddof=0)) if \"y_log1p\" in df.columns and df[\"y_log1p\"].std(ddof=0)>0 else 1.0\n",
    "    return mu, sigma\n",
    "\n",
    "mu, sigma = load_scaler_json(CONFIG[\"feature_files\"])\n",
    "\n",
    "def number_from_text(text: str) -> Optional[float]:\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", text)\n",
    "    return float(m.group(0)) if m else None\n",
    "\n",
    "def evaluate(model, tok, val_examples, mu, sigma, max_new_tokens=12):\n",
    "    model.eval()\n",
    "    preds_z, trues_z = [], []\n",
    "    for ex in val_examples:\n",
    "        ids = tok(ex[\"prompt\"], return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **ids, max_new_tokens=max_new_tokens, do_sample=False,\n",
    "                pad_token_id=tok.pad_token_id, eos_token_id=tok.eos_token_id\n",
    "            )\n",
    "        gen = tok.decode(out[0][ids[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        z_hat = number_from_text(gen)\n",
    "        if z_hat is None: continue\n",
    "        preds_z.append(z_hat)\n",
    "        trues_z.append(float(re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", ex[\"target\"])[0]))\n",
    "    if not preds_z:\n",
    "        return {\"val_MAE\": float(\"nan\"), \"val_RMSE\": float(\"nan\")}\n",
    "    preds_z = np.array(preds_z); trues_z = np.array(trues_z)\n",
    "    y_pred = np.expm1(preds_z * sigma + mu)\n",
    "    y_true = np.expm1(trues_z * sigma + mu)\n",
    "    return {\n",
    "        \"val_MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"val_RMSE\": math.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "val_pairs = [{\"prompt\": e[\"prompt\"], \"target\": e[\"target\"]} for e in val_text]\n",
    "metrics = evaluate(model, tokenizer, val_pairs, mu, sigma)\n",
    "print(\"Validation metrics:\", metrics)\n",
    "\n",
    "#  INFERENCE\n",
    "def forecast_next(raw_recent_volumes: List[float], last_feat_row: Dict[str,float], mu: float, sigma: float, k_decimals=5) -> float:\n",
    "    z_hist = z_hist_scaler.transform(np.log1p(np.array(raw_recent_volumes).reshape(-1,1))).reshape(-1)\n",
    "    hist_str = \", \".join(f\"{z:.4f}\" for z in z_hist[-CONFIG[\"context_len\"]:])\n",
    "    feats_str = \", \".join(f\"{k}={float(v):.4f}\" for k,v in last_feat_row.items()) if last_feat_row else \"none\"\n",
    "    prompt = f\"z_hist[{len(z_hist[-CONFIG['context_len']:])}]:{hist_str}\\nfeat:{feats_str}\\nnext_z:\"\n",
    "    ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**ids, max_new_tokens=12, do_sample=False,\n",
    "                             pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    gen = tokenizer.decode(out[0][ids[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    z_hat = number_from_text(gen)\n",
    "    if z_hat is None: raise RuntimeError(\"Model did not return a numeric answer.\")\n",
    "    return float(round(np.expm1(z_hat * sigma + mu), k_decimals))\n",
    "\n",
    "print(\"TinyLlama LoRA fine-tune done. Use forecast_next(...) for inference.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 934
    },
    "id": "0ZUkKJkj5trA",
    "outputId": "4078b066-cd4d-455d-dc55-bbe3a60e8d2b"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data shape: (6890, 53)\n",
      "        date    volume   y_trading    y_log1p  z_target\n",
      "0 1998-02-11  15819189  13524081.0  16.419983  0.920051\n",
      "1 1998-02-12  13524081   8694402.0  15.978190  0.337016\n",
      "2 1998-02-13   8694402  14912102.0  16.517684  1.048988\n",
      "3 1998-02-17  14912102  12788824.0  16.364082  0.846279\n",
      "4 1998-02-18  12788824  16986901.0  16.647953  1.220905\n",
      "Selected feature columns: ['ema28', 'ema7', 'roll14_mean', 'roll14_sum', 'roll28_sum', 'roll28_mean', 'roll7_mean', 'roll7_sum', 'year', 'roll7_min', 'roll14_min', 'roll28_min', 'lag1', 'lag2', 'roll7_max', 'fourier_t_sin1']\n",
      "Total samples: 6826\n",
      "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-1582264236.py:201: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='960' max='960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [960/960 1:13:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.812800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.349600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.342800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.341200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.336500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.345500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.338600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.330500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.332800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.335900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.332400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.325900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.322900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.320300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.315800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.317700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation metrics: {'val_MAE': 761753.6787408533, 'val_RMSE': 1541993.293025926}\n",
      "TinyLlama LoRA fine-tune done. Use forecast_next(...) for inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training a Numeric Value Head on TinyLlama with TinyLlama trained for 3 epochs."
   ],
   "metadata": {
    "id": "eDFq4nIk5Z7O"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os, re, math, json, numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# ---- config ----\n",
    "FEATURE_FILE = \"/content/features_trading_only_2.csv\"\n",
    "DATE_COL, VOL_COL, LABEL_COL = \"date\", \"volume\", \"z_target\"\n",
    "CTX, MAX_FEATURES, TRAIN_FRAC, MAX_LEN, EPOCHS = 64, 16, 0.9, 1024, 5\n",
    "LR, BATCH = 3e-4, 8\n",
    "DEVICE = next(model.parameters()).device\n",
    "\n",
    "# ---- reload features & splits ----\n",
    "df = pd.read_csv(FEATURE_FILE)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[DATE_COL]).sort_values(DATE_COL).reset_index(drop=True)\n",
    "df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "\n",
    "EXCL = {DATE_COL, LABEL_COL, VOL_COL, \"y_trading\", \"y_log1p\"}\n",
    "num = df.select_dtypes(include=[np.number]).copy()\n",
    "std = num.std(numeric_only=True)\n",
    "cand = [c for c in num.columns if c not in EXCL and std[c] > 0]\n",
    "corr = num[cand].corrwith(num[LABEL_COL]).abs().fillna(0.0)\n",
    "feature_cols = corr.sort_values(ascending=False).index.tolist()[:MAX_FEATURES]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "vol = df[VOL_COL].astype(float).values.reshape(-1,1)\n",
    "z_hist_scaler = StandardScaler()\n",
    "z_hist_series = z_hist_scaler.fit_transform(np.log1p(vol)).reshape(-1)\n",
    "\n",
    "def make_windows(df, z_hist, ctx, feat_cols, label_col):\n",
    "    X, Y = [], []\n",
    "    for t in range(ctx, len(df)):\n",
    "        X.append((z_hist[t-ctx:t].tolist(), df.iloc[t][feat_cols].to_dict()))\n",
    "        Y.append(float(df.iloc[t][label_col]))\n",
    "    return X, np.array(Y, dtype=np.float32)\n",
    "\n",
    "X_raw, Y = make_windows(df, z_hist_series, CTX, feature_cols, LABEL_COL)\n",
    "cut = int(len(X_raw)*TRAIN_FRAC)\n",
    "\n",
    "def ex_to_prompt(hist, feats):\n",
    "    hist_str  = \", \".join(f\"{x:.4f}\" for x in hist)\n",
    "    feats_str = \", \".join(f\"{k}={float(v):.4f}\" for k,v in feats.items()) if feats else \"none\"\n",
    "    return f\"z_hist[{len(hist)}]:{hist_str}\\nfeat:{feats_str}\\nnext_z:\"\n",
    "\n",
    "train_prompts = [ex_to_prompt(*X_raw[i]) for i in range(cut)]\n",
    "val_prompts   = [ex_to_prompt(*X_raw[i]) for i in range(cut, len(X_raw))]\n",
    "\n",
    "# scaler to invert\n",
    "mu, sigma = float(df[\"y_log1p\"].mean()), float(df[\"y_log1p\"].std(ddof=0))\n",
    "y_tr_z, y_va_z = Y[:cut], Y[cut:]\n",
    "y_tr, y_val = np.expm1(y_tr_z*sigma+mu), np.expm1(y_va_z*sigma+mu)\n",
    "\n",
    "# ---- dataset ----\n",
    "class TxtRegDS(Dataset):\n",
    "    def __init__(self, prompts, targets, tok, max_len=1024):\n",
    "        self.prompts, self.targets, self.tok, self.max_len = prompts, targets, tok, max_len\n",
    "    def __len__(self): return len(self.prompts)\n",
    "    def __getitem__(self, i):\n",
    "        x = self.tok(self.prompts[i], add_special_tokens=False, truncation=True, max_length=self.max_len)\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(x[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(x.get(\"attention_mask\", [1]*len(x[\"input_ids\"])), dtype=torch.long),\n",
    "            \"labels\": torch.tensor(self.targets[i], dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "train_ds = TxtRegDS(train_prompts, y_tr_z, tokenizer, MAX_LEN)\n",
    "val_ds   = TxtRegDS(val_prompts,   y_va_z, tokenizer, MAX_LEN)\n",
    "\n",
    "def pad_batch(batch, pad_id):\n",
    "    mx = max(len(b[\"input_ids\"]) for b in batch)\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([torch.cat([b[\"input_ids\"], torch.full((mx-len(b[\"input_ids\"]),), pad_id)]) for b in batch]),\n",
    "        \"attention_mask\": torch.stack([torch.cat([b[\"attention_mask\"], torch.zeros(mx-len(b[\"attention_mask\"]), dtype=torch.long)]) for b in batch]),\n",
    "        \"labels\": torch.stack([b[\"labels\"] for b in batch]),\n",
    "    }\n",
    "\n",
    "# ---- regression wrapper ----\n",
    "class LLMRegressor(nn.Module):\n",
    "    def __init__(self, base, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        for p in self.base.parameters(): p.requires_grad = False\n",
    "        hidden = self.base.config.hidden_size\n",
    "        self.head = nn.Sequential(nn.Dropout(dropout), nn.Linear(hidden, 1))\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        out = self.base(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                        output_hidden_states=True, use_cache=False)\n",
    "        h = out.hidden_states[-1]\n",
    "        idx = attention_mask.sum(dim=1) - 1\n",
    "        last_h = h[torch.arange(h.size(0), device=h.device), idx]\n",
    "        pred = self.head(last_h).squeeze(-1)\n",
    "        loss = F.mse_loss(pred, labels) if labels is not None else None\n",
    "        return {\"loss\": loss, \"logits\": pred.unsqueeze(-1)}\n",
    "\n",
    "reg_model = LLMRegressor(model).to(DEVICE)\n",
    "\n",
    "# ---- trainer ----\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/tinyllama_value_head\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    per_device_eval_batch_size=BATCH,\n",
    "    learning_rate=LR,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    do_eval=True,\n",
    "    bf16=(model.dtype==torch.bfloat16),\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    z_pred, z_true = logits.squeeze(-1), labels\n",
    "    y_pred = np.expm1(z_pred*sigma+mu); y_true = np.expm1(z_true*sigma+mu)\n",
    "    return {\n",
    "        \"MAE\": mean_absolute_error(y_true,y_pred),\n",
    "        \"RMSE\": math.sqrt(mean_squared_error(y_true,y_pred)),\n",
    "        \"MAPE%\": float(np.mean(np.abs((y_true-y_pred)/np.clip(y_true,1e-9,None)))*100)\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=reg_model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=lambda b: pad_batch(b, tokenizer.pad_token_id),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Value-head metrics (volumes):\", metrics)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IlwpkKYLaz8M",
    "outputId": "b3ce447e-e7df-4fe0-9dfa-9b9e09942ac1"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-3277967796.py:125: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3840' max='3840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3840/3840 08:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.569700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.314600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.367300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.320500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.295900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.358800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.323300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.307900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.293500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.309100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.282600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.319700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.309700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.273100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.265800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.268800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.270100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.299800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.275300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.244300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.264800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.226300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.248900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.233600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.226400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.247900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.242800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.275300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.227600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.200400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.240100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.185100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.196900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.241500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.188500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.192500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.253600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.201700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.194300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.147100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.228600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.190700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.184600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.194300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [86/86 00:09]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Value-head metrics (volumes): {'eval_loss': 0.511233925819397, 'eval_MAE': 1113996.75, 'eval_RMSE': 1550466.1081468372, 'eval_MAPE%': 60.99566650390625, 'eval_runtime': 9.9154, 'eval_samples_per_second': 68.883, 'eval_steps_per_second': 8.673, 'epoch': 5.0}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ---- Auto-find and loading the LoRA adapters, required for evaluation---\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "SEARCH_ROOTS = [\"/content/tinyllama_ts_lora\", \"/content\"]\n",
    "\n",
    "import os, time, torch, json\n",
    "from typing import Optional, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "try:\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    PEFT_OK = True\n",
    "except Exception:\n",
    "    PEFT_OK = False\n",
    "\n",
    "def find_latest_adapter(root_dirs) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Return dir containing BOTH adapter_model.* and adapter_config.json with the newest mtime.\n",
    "    Looks recursively under the given roots (handles trainer's checkpoint-* subdirs).\n",
    "    \"\"\"\n",
    "    best_dir, best_mtime = None, -1\n",
    "    for root in root_dirs:\n",
    "        if not os.path.exists(root):\n",
    "            continue\n",
    "        for cur, _, files in os.walk(root):\n",
    "            has_cfg = \"adapter_config.json\" in files\n",
    "            has_ad = any(f.startswith(\"adapter_model.\") for f in files)\n",
    "            if has_cfg and has_ad:\n",
    "                ad_files = [os.path.join(cur, f) for f in files if f.startswith(\"adapter_model.\")]\n",
    "                mtime = max(os.path.getmtime(p) for p in ad_files)\n",
    "                if mtime > best_mtime:\n",
    "                    best_mtime, best_dir = mtime, cur\n",
    "    return best_dir\n",
    "\n",
    "def find_merged_model(root_dirs) -> Optional[str]:\n",
    "    \"\"\"Return dir containing a full merged model (model.safetensors / pytorch_model.*).\"\"\"\n",
    "    for root in root_dirs:\n",
    "        if not os.path.exists(root):\n",
    "            continue\n",
    "        for cur, _, files in os.walk(root):\n",
    "            has_full = any(f in files for f in [\"model.safetensors\",\"pytorch_model.bin\",\"pytorch_model.safetensors\"])\n",
    "            if has_full:\n",
    "                return cur\n",
    "    return None\n",
    "\n",
    "def ensure_adapter_config(dirpath: str, base_model: str):\n",
    "    \"\"\"If adapter_config.json missing, write a minimal one that matches your LoRA training params.\"\"\"\n",
    "    cfg_path = os.path.join(dirpath, \"adapter_config.json\")\n",
    "    if os.path.exists(cfg_path):\n",
    "        return\n",
    "    lora_cfg = {\n",
    "        \"base_model_name_or_path\": base_model,\n",
    "        \"peft_type\": \"LORA\",\n",
    "        \"task_type\": \"CAUSAL_LM\",\n",
    "        \"r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\", \"inference_mode\": False,\n",
    "        \"target_modules\": [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    "    }\n",
    "    with open(cfg_path, \"w\") as f: json.dump(lora_cfg, f)\n",
    "    print(f\"[fix] wrote missing adapter_config.json at: {cfg_path}\")\n",
    "\n",
    "def load_ready_model_and_tokenizer(base_model: str, roots) -> Tuple[AutoModelForCausalLM, AutoTokenizer, str]:\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(base_model, use_fast=True, legacy=False)\n",
    "    if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "    dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8) else torch.float16\n",
    "\n",
    "\n",
    "    adir = find_latest_adapter(roots)\n",
    "    if adir and PEFT_OK:\n",
    "        ensure_adapter_config(adir, base_model)\n",
    "        try:\n",
    "            mdl = AutoPeftModelForCausalLM.from_pretrained(adir, torch_dtype=dtype, device_map=\"auto\")\n",
    "            mdl = mdl.merge_and_unload()\n",
    "            mdl.config.use_cache = False\n",
    "            print(f\"[ok] loaded & merged LoRA adapters from: {adir}\")\n",
    "            return mdl, tok, \"adapters_merged\"\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] adapter load failed at {adir}: {e}\")\n",
    "\n",
    "\n",
    "    mdir = find_merged_model(roots)\n",
    "    if mdir:\n",
    "        mdl = AutoModelForCausalLM.from_pretrained(mdir, torch_dtype=dtype, device_map=\"auto\")\n",
    "        mdl.config.use_cache = False\n",
    "        print(f\"[ok] loaded full merged model from: {mdir}\")\n",
    "        return mdl, tok, \"merged_full\"\n",
    "\n",
    "\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(base_model, torch_dtype=dtype, device_map=\"auto\")\n",
    "    mdl.config.use_cache = False\n",
    "    print(\"[WARN] adapters/merged not found — using BASE ONLY.\")\n",
    "    return mdl, tok, \"base_only\"\n",
    "\n",
    "model, tokenizer, mode = load_ready_model_and_tokenizer(BASE_MODEL, SEARCH_ROOTS)\n",
    "\n",
    "#  sanity checking:\n",
    "print(\"mode:\", mode)\n",
    "print(\"device:\", next(model.parameters()).device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oO_-G8FSXvy8",
    "outputId": "1d2c226e-4ce6-438d-a842-7bc6fa683472"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ok] loaded & merged LoRA adapters from: /content/tinyllama_ts_lora/checkpoint-960\n",
      "mode: adapters_merged\n",
      "device: cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model(TinyLlaMa) Evaluation -1"
   ],
   "metadata": {
    "id": "5Szg3jgNQvff"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TinyLLaMA trained on Feature_Group_1, with 3 epochs, Performance"
   ],
   "metadata": {
    "id": "M-PTPlC47D4N"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "FEATURE_FILE = \"/content/features_trading_only_2.csv\"\n",
    "DATE_COL, VOL_COL, LABEL_COL = \"date\", \"volume\", \"z_target\"\n",
    "CTX, MAX_FEATURES, TRAIN_FRAC, MAX_LEN = 64, 16, 0.9, 1024\n",
    "\n",
    "import os, re, math, json, numpy as np, pandas as pd, torch\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from transformers import LogitsProcessor\n",
    "\n",
    "# ---------- Load & prep features ----------\n",
    "df = pd.read_csv(FEATURE_FILE)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[DATE_COL]).sort_values(DATE_COL).reset_index(drop=True)\n",
    "df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "\n",
    "# Top-K by absolute Pearson (keep consistent with your training)\n",
    "EXCL = {DATE_COL, LABEL_COL, VOL_COL, \"y_trading\", \"y_log1p\"}\n",
    "num = df.select_dtypes(include=[np.number]).copy()\n",
    "std = num.std(numeric_only=True)\n",
    "non_const = std[std > 0].index.tolist()\n",
    "num = num[non_const]\n",
    "cand = [c for c in num.columns if c not in EXCL]\n",
    "if not cand: raise ValueError(\"No candidate numeric features found.\")\n",
    "corr = num[cand].corrwith(num[LABEL_COL]).abs().replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "feature_cols = corr.sort_values(ascending=False).index.tolist()[:MAX_FEATURES]\n",
    "\n",
    "# z-history from log1p(volume)\n",
    "vol = df[VOL_COL].astype(float).values.reshape(-1,1)\n",
    "z_hist_scaler = StandardScaler()\n",
    "z_hist_series = z_hist_scaler.fit_transform(np.log1p(vol)).reshape(-1)\n",
    "\n",
    "def make_windows(df, z_hist, ctx, feat_cols, label_col):\n",
    "    X, Y = [], []\n",
    "    for t in range(ctx, len(df)):\n",
    "        X.append((z_hist[t-ctx:t].tolist(), df.iloc[t][feat_cols].to_dict()))\n",
    "        Y.append(float(df.iloc[t][label_col]))\n",
    "    return X, np.array(Y, dtype=np.float32)\n",
    "\n",
    "X_raw, Y = make_windows(df, z_hist_series, CTX, feature_cols, LABEL_COL)\n",
    "cut = int(len(X_raw) * TRAIN_FRAC)\n",
    "val_text = []\n",
    "for hist, feats in X_raw[cut:]:\n",
    "    hist_str  = \", \".join(f\"{x:.4f}\" for x in hist)\n",
    "    feats_str = \", \".join(f\"{k}={float(v):.4f}\" for k,v in feats.items()) if feats else \"none\"\n",
    "    prompt = f\"z_hist[{len(hist)}]:{hist_str}\\nfeat:{feats_str}\\nnext_z:\"\n",
    "    val_text.append({\"prompt\": prompt})\n",
    "\n",
    "# scaler to invert to original units\n",
    "def load_scaler_json(feature_path: str):\n",
    "    base = os.path.dirname(feature_path)\n",
    "    cand = os.path.join(base, \"features_trading_only_scaler_2.json\")\n",
    "    if os.path.exists(cand):\n",
    "        with open(cand, \"r\") as f:\n",
    "            s = json.load(f)\n",
    "        return float(s[\"y_log1p_mean\"]), float(s[\"y_log1p_std\"])\n",
    "    mu = float(df[\"y_log1p\"].mean()) if \"y_log1p\" in df.columns else 0.0\n",
    "    std = df[\"y_log1p\"].std(ddof=0) if \"y_log1p\" in df.columns else 1.0\n",
    "    return mu, (float(std) if std and std > 0 else 1.0)\n",
    "\n",
    "mu, sigma = load_scaler_json(FEATURE_FILE)\n",
    "\n",
    "# y_val (ground truth) in original units\n",
    "y_va_z = Y[cut:]\n",
    "y_val  = np.expm1(y_va_z * sigma + mu)\n",
    "\n",
    "# ---------- Numeric-constrained decoding ----------\n",
    "class DigitsOnly(LogitsProcessor):\n",
    "    def __init__(self, tok, device):\n",
    "        allowed_chars = set(\"0123456789-+.eE \\n\")\n",
    "        ids = []\n",
    "        for i in range(tok.vocab_size):\n",
    "            s = tok.decode([i])\n",
    "            if s and set(s).issubset(allowed_chars):\n",
    "                ids.append(i)\n",
    "        self.allowed_ids = torch.tensor(ids, device=device)\n",
    "    def __call__(self, input_ids, scores):\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        mask[:, self.allowed_ids] = 0\n",
    "        return scores + mask\n",
    "\n",
    "digits_only = DigitsOnly(tokenizer, device=next(model.parameters()).device)\n",
    "num_pat = re.compile(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\")\n",
    "def number_from_text(s):\n",
    "    m = num_pat.search(s);\n",
    "    return float(m.group(0)) if m else None\n",
    "\n",
    "# ---------- Predict TinyLlama on validation ----------\n",
    "model.eval()\n",
    "preds_z = []\n",
    "for ex in val_text:\n",
    "    ids = tokenizer(ex[\"prompt\"], return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(next(model.parameters()).device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**ids, max_new_tokens=12, do_sample=False,\n",
    "                             logits_processor=[digits_only],\n",
    "                             pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    gen = tokenizer.decode(out[0][ids[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    z_hat = number_from_text(gen)\n",
    "    preds_z.append(np.nan if z_hat is None else z_hat)\n",
    "\n",
    "preds_z = np.array(preds_z, dtype=float)\n",
    "mask = ~np.isnan(preds_z)\n",
    "if mask.sum() == 0:\n",
    "    raise RuntimeError(\"Model returned no numeric outputs. Check prompts/decoding.\")\n",
    "y_pred = np.expm1(preds_z[mask] * sigma + mu)\n",
    "y_true = y_val[mask]\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "def metrics(y_true, y_pred):\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / np.clip(y_true, 1e-9, None))) * 100)\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE%\": mape}\n",
    "\n",
    "print(f\"Aligned eval samples: {len(y_true)} / {len(y_val)}\")\n",
    "print(\"TinyLlama (text→z→volume):\", metrics(y_true, y_pred))\n",
    "\n",
    "# ---------- Naive baselines on same span ----------\n",
    "y_all = np.expm1(Y * sigma + mu)\n",
    "tail_len = len(y_true)\n",
    "truth_tail = y_all[-tail_len:]\n",
    "\n",
    "def seasonal_naive(series, season=5):\n",
    "    yhat = np.roll(series, season); yhat[:season] = series[:season]; return yhat\n",
    "def moving_avg(series, k=7):\n",
    "    s = pd.Series(series)\n",
    "    return s.rolling(k, min_periods=1).mean().shift(1).bfill().to_numpy()\n",
    "\n",
    "sn = seasonal_naive(np.r_[y_all[:-tail_len], truth_tail])[-tail_len:]\n",
    "ma = moving_avg(np.r_[y_all[:-tail_len], truth_tail])[-tail_len:]\n",
    "print(\"Seasonal naive:\", metrics(truth_tail, sn))\n",
    "print(\"Moving average:\", metrics(truth_tail, ma))\n",
    "\n",
    "# ---------- Preview few predictions ----------\n",
    "for i in range(min(10, len(y_true))):\n",
    "    print(f\"{i:02d} | true={y_true[i]:.2f}  pred={y_pred[i]:.2f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9CIxeYUZb6p",
    "outputId": "98b551b4-d2a0-4f1e-bbde-7bd2bd25c5e6"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Aligned eval samples: 683 / 683\n",
      "TinyLlama (text→z→volume): {'MAE': 769220.187365776, 'RMSE': 1419827.828691292, 'MAPE%': 35.59028644550582}\n",
      "Seasonal naive: {'MAE': 1035122.125, 'RMSE': 1966850.8488850903, 'MAPE%': 43.81184768676758}\n",
      "Moving average: {'MAE': 783198.1818787911, 'RMSE': 1475405.823909972, 'MAPE%': 32.859275970257514}\n",
      "00 | true=2165886.50  pred=2899729.81\n",
      "01 | true=3933762.75  pred=3013983.48\n",
      "02 | true=2430223.25  pred=2679824.59\n",
      "03 | true=3374707.00  pred=2901927.90\n",
      "04 | true=3410402.75  pred=2749887.98\n",
      "05 | true=3193953.75  pred=3130365.99\n",
      "06 | true=11314851.00  pred=3106547.08\n",
      "07 | true=5218058.50  pred=3376760.99\n",
      "08 | true=3779608.00  pred=3376581.88\n",
      "09 | true=4183481.25  pred=3615025.68\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tiny LlaMa Model Evaluation-2"
   ],
   "metadata": {
    "id": "r5qcPQ8ESukt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Performance of TinyLlama trained on **Features_Group_1** with 5 epochs"
   ],
   "metadata": {
    "id": "Cf3cWROnA_wo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "FEATURE_FILE = \"/content/features_trading_only_2.csv\"\n",
    "DATE_COL, VOL_COL, LABEL_COL = \"date\", \"volume\", \"z_target\"\n",
    "CTX, MAX_FEATURES, TRAIN_FRAC, MAX_LEN = 64, 16, 0.9, 1024\n",
    "\n",
    "import os, re, math, json, numpy as np, pandas as pd, torch\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from transformers import LogitsProcessor\n",
    "\n",
    "# ---------- Loading & prep features ----------\n",
    "df = pd.read_csv(FEATURE_FILE)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[DATE_COL]).sort_values(DATE_COL).reset_index(drop=True)\n",
    "df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "\n",
    "# Top-K by absolute Pearson (keep consistent with your training)\n",
    "EXCL = {DATE_COL, LABEL_COL, VOL_COL, \"y_trading\", \"y_log1p\"}\n",
    "num = df.select_dtypes(include=[np.number]).copy()\n",
    "std = num.std(numeric_only=True)\n",
    "non_const = std[std > 0].index.tolist()\n",
    "num = num[non_const]\n",
    "cand = [c for c in num.columns if c not in EXCL]\n",
    "if not cand: raise ValueError(\"No candidate numeric features found.\")\n",
    "corr = num[cand].corrwith(num[LABEL_COL]).abs().replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "feature_cols = corr.sort_values(ascending=False).index.tolist()[:MAX_FEATURES]\n",
    "\n",
    "# z-history from log1p(volume)\n",
    "vol = df[VOL_COL].astype(float).values.reshape(-1,1)\n",
    "z_hist_scaler = StandardScaler()\n",
    "z_hist_series = z_hist_scaler.fit_transform(np.log1p(vol)).reshape(-1)\n",
    "\n",
    "def make_windows(df, z_hist, ctx, feat_cols, label_col):\n",
    "    X, Y = [], []\n",
    "    for t in range(ctx, len(df)):\n",
    "        X.append((z_hist[t-ctx:t].tolist(), df.iloc[t][feat_cols].to_dict()))\n",
    "        Y.append(float(df.iloc[t][label_col]))\n",
    "    return X, np.array(Y, dtype=np.float32)\n",
    "\n",
    "X_raw, Y = make_windows(df, z_hist_series, CTX, feature_cols, LABEL_COL)\n",
    "cut = int(len(X_raw) * TRAIN_FRAC)\n",
    "val_text = []\n",
    "for hist, feats in X_raw[cut:]:\n",
    "    hist_str  = \", \".join(f\"{x:.4f}\" for x in hist)\n",
    "    feats_str = \", \".join(f\"{k}={float(v):.4f}\" for k,v in feats.items()) if feats else \"none\"\n",
    "    prompt = f\"z_hist[{len(hist)}]:{hist_str}\\nfeat:{feats_str}\\nnext_z:\"\n",
    "    val_text.append({\"prompt\": prompt})\n",
    "\n",
    "# scaler to invert to original units\n",
    "def load_scaler_json(feature_path: str):\n",
    "    base = os.path.dirname(feature_path)\n",
    "    cand = os.path.join(base, \"features_trading_only_scaler_2.json\")\n",
    "    if os.path.exists(cand):\n",
    "        with open(cand, \"r\") as f:\n",
    "            s = json.load(f)\n",
    "        return float(s[\"y_log1p_mean\"]), float(s[\"y_log1p_std\"])\n",
    "    mu = float(df[\"y_log1p\"].mean()) if \"y_log1p\" in df.columns else 0.0\n",
    "    std = df[\"y_log1p\"].std(ddof=0) if \"y_log1p\" in df.columns else 1.0\n",
    "    return mu, (float(std) if std and std > 0 else 1.0)\n",
    "\n",
    "mu, sigma = load_scaler_json(FEATURE_FILE)\n",
    "\n",
    "# y_val (ground truth) in original units\n",
    "y_va_z = Y[cut:]\n",
    "y_val  = np.expm1(y_va_z * sigma + mu)\n",
    "\n",
    "# ---------- Numeric-constrained decoding ----------\n",
    "class DigitsOnly(LogitsProcessor):\n",
    "    def __init__(self, tok, device):\n",
    "        allowed_chars = set(\"0123456789-+.eE \\n\")\n",
    "        ids = []\n",
    "        for i in range(tok.vocab_size):\n",
    "            s = tok.decode([i])\n",
    "            if s and set(s).issubset(allowed_chars):\n",
    "                ids.append(i)\n",
    "        self.allowed_ids = torch.tensor(ids, device=device)\n",
    "    def __call__(self, input_ids, scores):\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        mask[:, self.allowed_ids] = 0\n",
    "        return scores + mask\n",
    "\n",
    "digits_only = DigitsOnly(tokenizer, device=next(model.parameters()).device)\n",
    "num_pat = re.compile(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\")\n",
    "def number_from_text(s):\n",
    "    m = num_pat.search(s);\n",
    "    return float(m.group(0)) if m else None\n",
    "\n",
    "# ---------- Predict TinyLlama on validation ----------\n",
    "model.eval()\n",
    "preds_z = []\n",
    "for ex in val_text:\n",
    "    ids = tokenizer(ex[\"prompt\"], return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(next(model.parameters()).device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**ids, max_new_tokens=12, do_sample=False,\n",
    "                             logits_processor=[digits_only],\n",
    "                             pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    gen = tokenizer.decode(out[0][ids[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    z_hat = number_from_text(gen)\n",
    "    preds_z.append(np.nan if z_hat is None else z_hat)\n",
    "\n",
    "preds_z = np.array(preds_z, dtype=float)\n",
    "mask = ~np.isnan(preds_z)\n",
    "if mask.sum() == 0:\n",
    "    raise RuntimeError(\"Model returned no numeric outputs. Check prompts/decoding.\")\n",
    "y_pred = np.expm1(preds_z[mask] * sigma + mu)\n",
    "y_true = y_val[mask]\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "def metrics(y_true, y_pred):\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / np.clip(y_true, 1e-9, None))) * 100)\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE%\": mape}\n",
    "\n",
    "print(f\"Aligned eval samples: {len(y_true)} / {len(y_val)}\")\n",
    "print(\"TinyLlama (text→z→volume):\", metrics(y_true, y_pred))\n",
    "\n",
    "# ---------- Naive baselines on same span ----------\n",
    "\n",
    "y_all = np.expm1(Y * sigma + mu)\n",
    "tail_len = len(y_true)\n",
    "truth_tail = y_all[-tail_len:]\n",
    "\n",
    "def seasonal_naive(series, season=5):\n",
    "    yhat = np.roll(series, season); yhat[:season] = series[:season]; return yhat\n",
    "def moving_avg(series, k=7):\n",
    "    s = pd.Series(series)\n",
    "    return s.rolling(k, min_periods=1).mean().shift(1).bfill().to_numpy()\n",
    "\n",
    "sn = seasonal_naive(np.r_[y_all[:-tail_len], truth_tail])[-tail_len:]\n",
    "ma = moving_avg(np.r_[y_all[:-tail_len], truth_tail])[-tail_len:]\n",
    "print(\"Seasonal naive:\", metrics(truth_tail, sn))\n",
    "print(\"Moving average:\", metrics(truth_tail, ma))\n",
    "\n",
    "# ---------- Preview few predictions ----------\n",
    "for i in range(min(10, len(y_true))):\n",
    "    print(f\"{i:02d} | true={y_true[i]:.2f}  pred={y_pred[i]:.2f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8lL0WCuUGrs3",
    "outputId": "585bb58e-0624-4ae8-8389-c59549598bcb"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Aligned eval samples: 683 / 683\n",
      "TinyLlama (text→z→volume): {'MAE': 728973.1755427595, 'RMSE': 1413633.286961244, 'MAPE%': 31.674799226962445}\n",
      "Seasonal naive: {'MAE': 1035122.125, 'RMSE': 1966850.8488850903, 'MAPE%': 43.81184768676758}\n",
      "Moving average: {'MAE': 783198.1818787911, 'RMSE': 1475405.823909972, 'MAPE%': 32.859275970257514}\n",
      "00 | true=2165886.50  pred=2899685.87\n",
      "01 | true=3933762.75  pred=3153865.62\n",
      "02 | true=2430223.25  pred=2751972.49\n",
      "03 | true=3374707.00  pred=3130057.64\n",
      "04 | true=3410402.75  pred=3152455.93\n",
      "05 | true=3193953.75  pred=3376709.81\n",
      "06 | true=11314851.00  pred=2886401.20\n",
      "07 | true=5218058.50  pred=3623252.84\n",
      "08 | true=3779608.00  pred=4216177.85\n",
      "09 | true=4183481.25  pred=3642550.04\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Performance Of TinyLlaMa, trained on Feature_Group_1 with 3 epochs, with Numerical Head Trained with 5 epochs."
   ],
   "metadata": {
    "id": "lPh2JAjORT0k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os, json, math, numpy as np, pandas as pd, torch\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -------- Config --------\n",
    "FEATURE_FILE = \"/content/features_trading_only_2.csv\"\n",
    "DATE_COL, VOL_COL, LABEL_COL = \"date\", \"volume\", \"z_target\"\n",
    "CTX, MAX_FEATURES, TRAIN_FRAC, MAX_LEN = 64, 16, 0.9, 1024\n",
    "DEVICE = next(reg_model.parameters()).device\n",
    "\n",
    "# -------- Data & split --------\n",
    "df = pd.read_csv(FEATURE_FILE)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[DATE_COL]).sort_values(DATE_COL).reset_index(drop=True)\n",
    "df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "\n",
    "EXCL = {DATE_COL, LABEL_COL, VOL_COL, \"y_trading\", \"y_log1p\"}\n",
    "num = df.select_dtypes(include=[np.number]).copy()\n",
    "std = num.std(numeric_only=True)\n",
    "cand = [c for c in num.columns if c not in EXCL and std[c] > 0]\n",
    "corr = num[cand].corrwith(num[LABEL_COL]).abs().fillna(0.0)\n",
    "feature_cols = corr.sort_values(ascending=False).index.tolist()[:MAX_FEATURES]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "vol = df[VOL_COL].astype(float).values.reshape(-1,1)\n",
    "z_hist_scaler = StandardScaler()\n",
    "z_hist_series = z_hist_scaler.fit_transform(np.log1p(vol)).reshape(-1)\n",
    "\n",
    "def make_windows(df, z_hist, ctx, feat_cols, label_col):\n",
    "    X, Y = [], []\n",
    "    for t in range(ctx, len(df)):\n",
    "        X.append((z_hist[t-ctx:t].tolist(), df.iloc[t][feat_cols].to_dict()))\n",
    "        Y.append(float(df.iloc[t][label_col]))\n",
    "    return X, np.array(Y, dtype=np.float32)\n",
    "\n",
    "X_raw, Y = make_windows(df, z_hist_series, CTX, feature_cols, LABEL_COL)\n",
    "cut = int(len(X_raw) * TRAIN_FRAC)\n",
    "\n",
    "def to_prompt(hist, feats):\n",
    "    hist_str = \", \".join(f\"{x:.4f}\" for x in hist)\n",
    "    feats_str = \", \".join(f\"{k}={float(v):.4f}\" for k,v in feats.items()) if feats else \"none\"\n",
    "    return f\"z_hist[{len(hist)}]:{hist_str}\\nfeat:{feats_str}\\nnext_z:\"\n",
    "\n",
    "val_prompts = [to_prompt(*X_raw[i]) for i in range(cut, len(X_raw))]\n",
    "\n",
    "# scaler to invert to original units\n",
    "def load_scaler_json(path):\n",
    "    base = os.path.dirname(path)\n",
    "    cand = os.path.join(base, \"features_trading_only_scaler_2.json\")\n",
    "    if os.path.exists(cand):\n",
    "        with open(cand, \"r\") as f: s = json.load(f)\n",
    "        return float(s[\"y_log1p_mean\"]), float(s[\"y_log1p_std\"])\n",
    "    mu = float(df[\"y_log1p\"].mean()) if \"y_log1p\" in df.columns else 0.0\n",
    "    sig = df[\"y_log1p\"].std(ddof=0) if \"y_log1p\" in df.columns else 1.0\n",
    "    return mu, (float(sig) if sig and sig > 0 else 1.0)\n",
    "\n",
    "mu, sigma = load_scaler_json(FEATURE_FILE)\n",
    "y_va_z = Y[cut:]\n",
    "y_val  = np.expm1(y_va_z * sigma + mu)\n",
    "\n",
    "# -------- LLM + value head predictions --------\n",
    "reg_model.eval()\n",
    "z_pred = []\n",
    "with torch.no_grad():\n",
    "    for p in val_prompts:\n",
    "        tok = tokenizer(p, add_special_tokens=False, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "        tok = {k: v.to(DEVICE) for k,v in tok.items()}\n",
    "        out = reg_model(**tok)\n",
    "        z_pred.append(out[\"logits\"].squeeze(-1).squeeze(0).detach().cpu().item())\n",
    "\n",
    "z_pred = np.array(z_pred, dtype=float)\n",
    "y_pred_llm = np.expm1(z_pred * sigma + mu)\n",
    "\n",
    "# -------- Naive baselines --------\n",
    "y_all = np.expm1(Y * sigma + mu)\n",
    "tail_len = len(y_pred_llm)\n",
    "truth_tail = y_all[-tail_len:]\n",
    "\n",
    "def seasonal_naive(series, season=5):\n",
    "    yhat = np.roll(series, season); yhat[:season] = series[:season]; return yhat\n",
    "\n",
    "def moving_avg(series, k=7):\n",
    "    s = pd.Series(series)\n",
    "    return s.rolling(k, min_periods=1).mean().shift(1).bfill().to_numpy()\n",
    "\n",
    "sn = seasonal_naive(np.r_[y_all[:-tail_len], truth_tail])[-tail_len:]\n",
    "ma = moving_avg(np.r_[y_all[:-tail_len], truth_tail])[-tail_len:]\n",
    "\n",
    "# -------- Metrics --------\n",
    "def metrics(y_true, y_pred):\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / np.clip(y_true, 1e-9, None))) * 100)\n",
    "    return mae, rmse, mape\n",
    "\n",
    "rows = []\n",
    "rows.append((\"LLM + Value Head\",) + metrics(y_val, y_pred_llm))\n",
    "rows.append((\"Seasonal Naive (lag=5)\",) + metrics(y_val, sn))\n",
    "rows.append((\"Moving Average (k=7)\",) + metrics(y_val, ma))\n",
    "\n",
    "df_res = pd.DataFrame(rows, columns=[\"Model\",\"MAE\",\"RMSE\",\"MAPE%\"])\n",
    "print(df_res.to_string(index=False))\n",
    "\n",
    "# -------- Top-10 absolute-error misses --------\n",
    "err = np.abs(y_pred_llm - y_val)\n",
    "idx = np.argsort(-err)[:10]\n",
    "print(\"\\nTop-10 misses (LLM + Value Head):\")\n",
    "for j in idx:\n",
    "    print(f\"{j:03d} | true={y_val[j]:.0f}  pred={y_pred_llm[j]:.0f}  abs_err={err[j]:.0f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8tSMtbDddWOn",
    "outputId": "580f2e44-04c2-41f4-82b7-3bada90b17d2"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                 Model          MAE         RMSE     MAPE%\n",
      "      LLM + Value Head 1.119315e+06 1.557513e+06 61.201292\n",
      "Seasonal Naive (lag=5) 1.035122e+06 1.966851e+06 43.811848\n",
      "  Moving Average (k=7) 7.831982e+05 1.475406e+06 32.859276\n",
      "\n",
      "Top-10 misses (LLM + Value Head):\n",
      "322 | true=22640160  pred=4638328  abs_err=18001832\n",
      "072 | true=13930385  pred=4345909  abs_err=9584476\n",
      "006 | true=11314851  pred=4384672  abs_err=6930179\n",
      "634 | true=9685454  pred=2787773  abs_err=6897681\n",
      "385 | true=11448904  pred=4672778  abs_err=6776126\n",
      "576 | true=9816966  pred=3860656  abs_err=5956310\n",
      "192 | true=7737242  pred=2461880  abs_err=5275362\n",
      "235 | true=7526871  pred=2804326  abs_err=4722545\n",
      "261 | true=7557361  pred=3028664  abs_err=4528697\n",
      "130 | true=7017421  pred=2596604  abs_err=4420817\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using XGBoost for selecting top-k features"
   ],
   "metadata": {
    "id": "3RKPE6JfgSdx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q xgboost"
   ],
   "metadata": {
    "id": "D5eTxo-agaIf"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# ---- config ----\n",
    "FEATURE_FILE = \"/content/features_trading_only_2.csv\"\n",
    "DATE_COL, VOL_COL, LABEL_COL = \"date\", \"volume\", \"z_target\"\n",
    "K = 16  # top-k features to keep\n",
    "\n",
    "# ---- load features ----\n",
    "df = pd.read_csv(FEATURE_FILE)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[DATE_COL]).sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "# numeric features only\n",
    "exclude = {DATE_COL, LABEL_COL, VOL_COL, \"y_trading\", \"y_log1p\"}\n",
    "num = df.select_dtypes(include=[np.number]).copy()\n",
    "cand = [c for c in num.columns if c not in exclude]\n",
    "\n",
    "X = num[cand].fillna(0).values\n",
    "y = df[LABEL_COL].values\n",
    "\n",
    "# ---- train/val split ----\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ---- fit XGBoost regressor ----\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=cand)\n",
    "dval   = xgb.DMatrix(X_val,   label=y_val,   feature_names=cand)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"max_depth\": 4,\n",
    "    \"eta\": 0.1,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_boost_round=200, evals=[(dval,\"val\")],\n",
    "                early_stopping_rounds=20, verbose_eval=False)\n",
    "\n",
    "# ---- feature importance ----\n",
    "imp_gain = bst.get_score(importance_type=\"gain\")\n",
    "imp_sorted = sorted(imp_gain.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nTop-{K} features by XGBoost importance:\")\n",
    "top_features = [f for f,_ in imp_sorted[:K]]\n",
    "for rank,(feat,score) in enumerate(imp_sorted[:K],1):\n",
    "    print(f\"{rank:02d}. {feat:25s} gain={score:.4f}\")\n",
    "\n",
    "# save to list for later use!\n",
    "feature_cols_xgb = top_features\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAkSoUzjgSMV",
    "outputId": "060d23a7-e5ba-4359-8e5b-a03fe97b13bf"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Top-16 features by XGBoost importance:\n",
      "01. year                      gain=91.3184\n",
      "02. ema7                      gain=79.9638\n",
      "03. fourier_t_sin1            gain=70.3679\n",
      "04. ema28                     gain=59.0522\n",
      "05. roll7_sum                 gain=35.2179\n",
      "06. roll7_mean                gain=27.8385\n",
      "07. roll14_mean               gain=9.5542\n",
      "08. roll7_min                 gain=8.9750\n",
      "09. lag1                      gain=6.5701\n",
      "10. ret7                      gain=4.4075\n",
      "11. ret1                      gain=3.8452\n",
      "12. dom                       gain=2.9399\n",
      "13. week                      gain=2.6481\n",
      "14. month                     gain=2.4693\n",
      "15. gap_days_next             gain=2.4107\n",
      "16. fourier_y_sin1            gain=2.1528\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fine-tuning TinyLlama On Feature_Group_2, 3 epochs"
   ],
   "metadata": {
    "id": "6nxsaq68SP4d"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os, re, math, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import set_seed\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============================  CONFIG  ============================\n",
    "CONFIG = {\n",
    "    \"feature_files\": [\"/content/features_trading_only_2.csv\"],\n",
    "    \"date_col\": \"date\",\n",
    "    \"vol_col\": \"volume\",\n",
    "    \"label_col\": \"z_target\",\n",
    "    \"context_len\": 64,\n",
    "    \"max_features\": 16,\n",
    "    \"model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"seed\": 42,\n",
    "    \"train_frac\": 0.9,\n",
    "    \"epochs\": 3,\n",
    "    \"lr\": 1e-4,\n",
    "    \"train_bs\": 2,\n",
    "    \"grad_accum\": 16,\n",
    "    \"max_length\": 1024,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"output_dir\": \"/content/tinyllama_ts_lora\",\n",
    "    \"bf16\": True,\n",
    "}\n",
    "\n",
    "set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "# ======================  LOAD & MERGE FEATURES  ==================\n",
    "def load_and_merge(paths: List[str], date_col: str):\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        df = pd.read_csv(p)\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[date_col])\n",
    "        dfs.append(df)\n",
    "    all_df = pd.concat(dfs, axis=0, ignore_index=True).sort_values(date_col).reset_index(drop=True)\n",
    "    all_df = all_df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "    return all_df\n",
    "\n",
    "df = load_and_merge(CONFIG[\"feature_files\"], CONFIG[\"date_col\"])\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(df[[CONFIG[\"date_col\"], CONFIG[\"vol_col\"], \"y_trading\", \"y_log1p\", CONFIG[\"label_col\"]]].head())\n",
    "\n",
    "# =====================  SELECT TOP-K FEATURES  ====================\n",
    "EXCLUDE_COLS = {CONFIG[\"date_col\"], CONFIG[\"label_col\"], CONFIG[\"vol_col\"], \"y_trading\", \"y_log1p\"}\n",
    "num = df.select_dtypes(include=[np.number]).copy()\n",
    "std = num.std(numeric_only=True)\n",
    "non_const = std[std > 0].index.tolist()\n",
    "num = num[non_const]\n",
    "cand = [c for c in num.columns if c not in EXCLUDE_COLS]\n",
    "if not cand:\n",
    "    raise ValueError(\"No candidate numeric features found after exclusions.\")\n",
    "# corr = num[cand].corrwith(num[CONFIG[\"label_col\"]]).abs().replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "# feature_cols = corr.sort_values(ascending=False).index.tolist()[:CONFIG[\"max_features\"]]\n",
    "feature_cols = [\n",
    "    \"year\",\"ema7\",\"fourier_t_sin1\",\"ema28\",\"roll7_sum\",\"roll7_mean\",\n",
    "    \"roll14_mean\",\"roll7_min\",\"lag1\",\"ret7\",\"ret1\",\n",
    "    \"dom\",\"week\",\"month\",\"gap_days_next\",\"fourier_y_sin1\"\n",
    "]\n",
    "print(\"Selected feature columns:\", feature_cols)\n",
    "\n",
    "# ====================  BUILD z_history SERIES  ====================\n",
    "\n",
    "vol = df[CONFIG[\"vol_col\"]].astype(float).values.reshape(-1,1)\n",
    "z_hist_scaler = StandardScaler()\n",
    "z_hist_series = z_hist_scaler.fit_transform(np.log1p(vol)).reshape(-1)\n",
    "\n",
    "# ====================  BUILD WINDOWS  ====================\n",
    "def make_windows(df: pd.DataFrame, z_hist: np.ndarray, ctx: int, feat_cols: List[str], label_col: str):\n",
    "    X, Y = [], []\n",
    "    n = len(df)\n",
    "    for t in range(ctx, n):\n",
    "        hist = z_hist[t-ctx:t].tolist()\n",
    "        feats = df.iloc[t][feat_cols].to_dict()\n",
    "        y = float(df.iloc[t][label_col])\n",
    "        X.append((hist, feats))\n",
    "        Y.append(y)\n",
    "    return X, np.array(Y, dtype=np.float32)\n",
    "\n",
    "X_raw, Y = make_windows(df, z_hist_series, CONFIG[\"context_len\"], feature_cols, CONFIG[\"label_col\"])\n",
    "print(\"Total samples:\", len(X_raw))\n",
    "\n",
    "# ===================  TRAIN / VAL SPLIT  ==========================\n",
    "N = len(X_raw)\n",
    "cut = int(N * CONFIG[\"train_frac\"])\n",
    "train_idx = np.arange(0, cut)\n",
    "val_idx = np.arange(cut, N)\n",
    "\n",
    "def ex_to_text(hist, feats, y_z):\n",
    "    hist_str = \", \".join(f\"{x:.4f}\" for x in hist)\n",
    "    feats_str = \", \".join(f\"{k}={float(v):.4f}\" for k,v in feats.items()) if feats else \"none\"\n",
    "    prompt = f\"z_hist[{len(hist)}]:{hist_str}\\nfeat:{feats_str}\\nnext_z:\"\n",
    "    target = f\"{y_z:.5f}\\n\"\n",
    "    return {\"prompt\": prompt, \"target\": target}\n",
    "\n",
    "train_text = [ex_to_text(*X_raw[i], Y[i]) for i in train_idx]\n",
    "val_text   = [ex_to_text(*X_raw[i], Y[i]) for i in val_idx]\n",
    "\n",
    "# ===================  TOKENIZER / MODEL  =========================\n",
    "model_name = CONFIG[\"model_name\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "torch_dtype = (\n",
    "    torch.bfloat16\n",
    "    if (CONFIG[\"bf16\"] and torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8)\n",
    "    else torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",   # no extra installs\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# LoRA targets for TinyLlama blocks\n",
    "lora_cfg = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"], lora_alpha=CONFIG[\"lora_alpha\"], lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ===================  DATASET  ========================\n",
    "class TxtDS(Dataset):\n",
    "    def __init__(self, examples, tok, max_len=1024):\n",
    "        self.ex = examples; self.tok = tok; self.max_len = max_len\n",
    "    def __len__(self): return len(self.ex)\n",
    "    def __getitem__(self, i):\n",
    "        e = self.ex[i]\n",
    "        p_ids = self.tok(e[\"prompt\"], add_special_tokens=False)[\"input_ids\"]\n",
    "        t_ids = self.tok(e[\"target\"], add_special_tokens=False)[\"input_ids\"]\n",
    "        ids = p_ids + t_ids\n",
    "        if len(ids) > self.max_len:\n",
    "            overflow = len(ids) - self.max_len\n",
    "            keep_p = max(0, len(p_ids) - overflow)\n",
    "            ids = p_ids[-keep_p:] + t_ids\n",
    "        p_len = min(len(p_ids), len(ids) - len(t_ids))\n",
    "        labels = [-100]*p_len + ids[p_len:]\n",
    "        attn = [1]*len(ids)\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def pad_batch(batch, pad_id):\n",
    "    mx = max(len(b[\"input_ids\"]) for b in batch)\n",
    "    out = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    for b in batch:\n",
    "        pad_n = mx - len(b[\"input_ids\"])\n",
    "        out[\"input_ids\"].append(torch.cat([b[\"input_ids\"], torch.full((pad_n,), pad_id, dtype=torch.long)]))\n",
    "        out[\"attention_mask\"].append(torch.cat([b[\"attention_mask\"], torch.zeros(pad_n, dtype=torch.long)]))\n",
    "        out[\"labels\"].append(torch.cat([b[\"labels\"], torch.full((pad_n,), -100, dtype=torch.long)]))\n",
    "    return {k: torch.stack(v) for k,v in out.items()}\n",
    "\n",
    "def collate_fn(features):\n",
    "    return pad_batch(features, tokenizer.pad_token_id)\n",
    "\n",
    "train_ds = TxtDS(train_text, tokenizer, CONFIG[\"max_length\"])\n",
    "val_ds   = TxtDS(val_text, tokenizer, CONFIG[\"max_length\"])\n",
    "\n",
    "# ===================  TRAIN  ==========================\n",
    "args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"train_bs\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"train_bs\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"grad_accum\"],\n",
    "    learning_rate=CONFIG[\"lr\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=(torch_dtype==torch.bfloat16),\n",
    "    fp16=(torch_dtype==torch.float16),\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=\"none\",\n",
    "    do_eval=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "#  EVAL\n",
    "\n",
    "def load_scaler_json(paths):\n",
    "    base = os.path.dirname(paths[0])\n",
    "    cand = os.path.join(base, \"features_trading_only_scaler_2.json\")\n",
    "    if os.path.exists(cand):\n",
    "        with open(cand, \"r\") as f:\n",
    "            s = json.load(f)\n",
    "        return float(s[\"y_log1p_mean\"]), float(s[\"y_log1p_std\"])\n",
    "    mu = float(df[\"y_log1p\"].mean()) if \"y_log1p\" in df.columns else 0.0\n",
    "    sigma = float(df[\"y_log1p\"].std(ddof=0)) if \"y_log1p\" in df.columns and df[\"y_log1p\"].std(ddof=0)>0 else 1.0\n",
    "    return mu, sigma\n",
    "\n",
    "mu, sigma = load_scaler_json(CONFIG[\"feature_files\"])\n",
    "\n",
    "def number_from_text(text: str) -> Optional[float]:\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", text)\n",
    "    return float(m.group(0)) if m else None\n",
    "\n",
    "def evaluate(model, tok, val_examples, mu, sigma, max_new_tokens=12):\n",
    "    model.eval()\n",
    "    preds_z, trues_z = [], []\n",
    "    for ex in val_examples:\n",
    "        ids = tok(ex[\"prompt\"], return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **ids, max_new_tokens=max_new_tokens, do_sample=False,\n",
    "                pad_token_id=tok.pad_token_id, eos_token_id=tok.eos_token_id\n",
    "            )\n",
    "        gen = tok.decode(out[0][ids[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        z_hat = number_from_text(gen)\n",
    "        if z_hat is None: continue\n",
    "        preds_z.append(z_hat)\n",
    "        trues_z.append(float(re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", ex[\"target\"])[0]))\n",
    "    if not preds_z:\n",
    "        return {\"val_MAE\": float(\"nan\"), \"val_RMSE\": float(\"nan\")}\n",
    "    preds_z = np.array(preds_z); trues_z = np.array(trues_z)\n",
    "    y_pred = np.expm1(preds_z * sigma + mu)\n",
    "    y_true = np.expm1(trues_z * sigma + mu)\n",
    "    return {\n",
    "        \"val_MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"val_RMSE\": math.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "val_pairs = [{\"prompt\": e[\"prompt\"], \"target\": e[\"target\"]} for e in val_text]\n",
    "metrics = evaluate(model, tokenizer, val_pairs, mu, sigma)\n",
    "print(\"Validation metrics:\", metrics)\n",
    "\n",
    "# ===================  INFERENCE  ==========================\n",
    "def forecast_next(raw_recent_volumes: List[float], last_feat_row: Dict[str,float], mu: float, sigma: float, k_decimals=5) -> float:\n",
    "    z_hist = z_hist_scaler.transform(np.log1p(np.array(raw_recent_volumes).reshape(-1,1))).reshape(-1)\n",
    "    hist_str = \", \".join(f\"{z:.4f}\" for z in z_hist[-CONFIG[\"context_len\"]:])\n",
    "    feats_str = \", \".join(f\"{k}={float(v):.4f}\" for k,v in last_feat_row.items()) if last_feat_row else \"none\"\n",
    "    prompt = f\"z_hist[{len(z_hist[-CONFIG['context_len']:])}]:{hist_str}\\nfeat:{feats_str}\\nnext_z:\"\n",
    "    ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**ids, max_new_tokens=12, do_sample=False,\n",
    "                             pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    gen = tokenizer.decode(out[0][ids[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    z_hat = number_from_text(gen)\n",
    "    if z_hat is None: raise RuntimeError(\"Model did not return a numeric answer.\")\n",
    "    return float(round(np.expm1(z_hat * sigma + mu), k_decimals))\n",
    "\n",
    "print(\"TinyLlama LoRA fine-tune done. Use forecast_next(...) for inference.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "id": "Bt4oqGMIgoDm",
    "outputId": "d0c82881-b370-4e59-f3f2-5c422d3425bd"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data shape: (6890, 53)\n",
      "        date    volume   y_trading    y_log1p  z_target\n",
      "0 1998-02-11  15819189  13524081.0  16.419983  0.920051\n",
      "1 1998-02-12  13524081   8694402.0  15.978190  0.337016\n",
      "2 1998-02-13   8694402  14912102.0  16.517684  1.048988\n",
      "3 1998-02-17  14912102  12788824.0  16.364082  0.846279\n",
      "4 1998-02-18  12788824  16986901.0  16.647953  1.220905\n",
      "Selected feature columns: ['year', 'ema7', 'fourier_t_sin1', 'ema28', 'roll7_sum', 'roll7_mean', 'roll14_mean', 'roll7_min', 'lag1', 'ret7', 'ret1', 'dom', 'week', 'month', 'gap_days_next', 'fourier_y_sin1']\n",
      "Total samples: 6826\n",
      "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-2038561240.py:206: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='576' max='576' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [576/576 43:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.754100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.354300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.346300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.339800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.337200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.334800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.324700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.325900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.329500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation metrics: {'val_MAE': 713283.7563821452, 'val_RMSE': 1367959.9324602024}\n",
      "TinyLlama LoRA fine-tune done. Use forecast_next(...) for inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tiny LlaMa Model Evaluation -3"
   ],
   "metadata": {
    "id": "M83VVC0ASiGQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Performance Of TinyLlama trained on Feature_Group_2, trained with 3 Epoch"
   ],
   "metadata": {
    "id": "EMklgHwZSb9r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "FEATURE_FILE = \"/content/features_trading_only_2.csv\"\n",
    "DATE_COL, VOL_COL, LABEL_COL = \"date\", \"volume\", \"z_target\"\n",
    "CTX, MAX_FEATURES, TRAIN_FRAC, MAX_LEN = 64, 16, 0.9, 1024\n",
    "\n",
    "import os, re, math, json, numpy as np, pandas as pd, torch\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from transformers import LogitsProcessor\n",
    "\n",
    "# ---------- Load & prep features ----------\n",
    "df = pd.read_csv(FEATURE_FILE)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[DATE_COL]).sort_values(DATE_COL).reset_index(drop=True)\n",
    "df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "\n",
    "# Top-K by absolute Pearson (keep consistent with your training)\n",
    "EXCL = {DATE_COL, LABEL_COL, VOL_COL, \"y_trading\", \"y_log1p\"}\n",
    "num = df.select_dtypes(include=[np.number]).copy()\n",
    "std = num.std(numeric_only=True)\n",
    "non_const = std[std > 0].index.tolist()\n",
    "num = num[non_const]\n",
    "cand = [c for c in num.columns if c not in EXCL]\n",
    "if not cand: raise ValueError(\"No candidate numeric features found.\")\n",
    "corr = num[cand].corrwith(num[LABEL_COL]).abs().replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "feature_cols = corr.sort_values(ascending=False).index.tolist()[:MAX_FEATURES]\n",
    "\n",
    "# z-history from log1p(volume)\n",
    "vol = df[VOL_COL].astype(float).values.reshape(-1,1)\n",
    "z_hist_scaler = StandardScaler()\n",
    "z_hist_series = z_hist_scaler.fit_transform(np.log1p(vol)).reshape(-1)\n",
    "\n",
    "def make_windows(df, z_hist, ctx, feat_cols, label_col):\n",
    "    X, Y = [], []\n",
    "    for t in range(ctx, len(df)):\n",
    "        X.append((z_hist[t-ctx:t].tolist(), df.iloc[t][feat_cols].to_dict()))\n",
    "        Y.append(float(df.iloc[t][label_col]))\n",
    "    return X, np.array(Y, dtype=np.float32)\n",
    "\n",
    "X_raw, Y = make_windows(df, z_hist_series, CTX, feature_cols, LABEL_COL)\n",
    "cut = int(len(X_raw) * TRAIN_FRAC)\n",
    "val_text = []\n",
    "for hist, feats in X_raw[cut:]:\n",
    "    hist_str  = \", \".join(f\"{x:.4f}\" for x in hist)\n",
    "    feats_str = \", \".join(f\"{k}={float(v):.4f}\" for k,v in feats.items()) if feats else \"none\"\n",
    "    prompt = f\"z_hist[{len(hist)}]:{hist_str}\\nfeat:{feats_str}\\nnext_z:\"\n",
    "    val_text.append({\"prompt\": prompt})\n",
    "\n",
    "# scaler to invert to original units\n",
    "def load_scaler_json(feature_path: str):\n",
    "    base = os.path.dirname(feature_path)\n",
    "    cand = os.path.join(base, \"features_trading_only_scaler_2.json\")\n",
    "    if os.path.exists(cand):\n",
    "        with open(cand, \"r\") as f:\n",
    "            s = json.load(f)\n",
    "        return float(s[\"y_log1p_mean\"]), float(s[\"y_log1p_std\"])\n",
    "    mu = float(df[\"y_log1p\"].mean()) if \"y_log1p\" in df.columns else 0.0\n",
    "    std = df[\"y_log1p\"].std(ddof=0) if \"y_log1p\" in df.columns else 1.0\n",
    "    return mu, (float(std) if std and std > 0 else 1.0)\n",
    "\n",
    "mu, sigma = load_scaler_json(FEATURE_FILE)\n",
    "\n",
    "# y_val (ground truth) in original units\n",
    "y_va_z = Y[cut:]\n",
    "y_val  = np.expm1(y_va_z * sigma + mu)\n",
    "\n",
    "# ---------- Numeric-constrained decoding ----------\n",
    "class DigitsOnly(LogitsProcessor):\n",
    "    def __init__(self, tok, device):\n",
    "        allowed_chars = set(\"0123456789-+.eE \\n\")\n",
    "        ids = []\n",
    "        for i in range(tok.vocab_size):\n",
    "            s = tok.decode([i])\n",
    "            if s and set(s).issubset(allowed_chars):\n",
    "                ids.append(i)\n",
    "        self.allowed_ids = torch.tensor(ids, device=device)\n",
    "    def __call__(self, input_ids, scores):\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        mask[:, self.allowed_ids] = 0\n",
    "        return scores + mask\n",
    "\n",
    "digits_only = DigitsOnly(tokenizer, device=next(model.parameters()).device)\n",
    "num_pat = re.compile(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\")\n",
    "def number_from_text(s):\n",
    "    m = num_pat.search(s);\n",
    "    return float(m.group(0)) if m else None\n",
    "\n",
    "# ---------- Predict TinyLlama on validation ----------\n",
    "model.eval()\n",
    "preds_z = []\n",
    "for ex in val_text:\n",
    "    ids = tokenizer(ex[\"prompt\"], return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(next(model.parameters()).device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**ids, max_new_tokens=12, do_sample=False,\n",
    "                             logits_processor=[digits_only],\n",
    "                             pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    gen = tokenizer.decode(out[0][ids[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    z_hat = number_from_text(gen)\n",
    "    preds_z.append(np.nan if z_hat is None else z_hat)\n",
    "\n",
    "preds_z = np.array(preds_z, dtype=float)\n",
    "mask = ~np.isnan(preds_z)\n",
    "if mask.sum() == 0:\n",
    "    raise RuntimeError(\"Model returned no numeric outputs. Check prompts/decoding.\")\n",
    "y_pred = np.expm1(preds_z[mask] * sigma + mu)\n",
    "y_true = y_val[mask]\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "def metrics(y_true, y_pred):\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / np.clip(y_true, 1e-9, None))) * 100)\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE%\": mape}\n",
    "\n",
    "print(f\"Aligned eval samples: {len(y_true)} / {len(y_val)}\")\n",
    "print(\"TinyLlama (text→z→volume):\", metrics(y_true, y_pred))\n",
    "\n",
    "# ---------- Naive baselines on same span ----------\n",
    "\n",
    "y_all = np.expm1(Y * sigma + mu)\n",
    "tail_len = len(y_true)\n",
    "truth_tail = y_all[-tail_len:]\n",
    "\n",
    "def seasonal_naive(series, season=5):\n",
    "    yhat = np.roll(series, season); yhat[:season] = series[:season]; return yhat\n",
    "def moving_avg(series, k=7):\n",
    "    s = pd.Series(series)\n",
    "    return s.rolling(k, min_periods=1).mean().shift(1).bfill().to_numpy()\n",
    "\n",
    "sn = seasonal_naive(np.r_[y_all[:-tail_len], truth_tail])[-tail_len:]\n",
    "ma = moving_avg(np.r_[y_all[:-tail_len], truth_tail])[-tail_len:]\n",
    "print(\"Seasonal naive:\", metrics(truth_tail, sn))\n",
    "print(\"Moving average:\", metrics(truth_tail, ma))\n",
    "\n",
    "# ---------- Preview few predictions ----------\n",
    "for i in range(min(10, len(y_true))):\n",
    "    print(f\"{i:02d} | true={y_true[i]:.2f}  pred={y_pred[i]:.2f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOuz2Bd7tHz7",
    "outputId": "d22dd59f-d50b-4c0b-c183-eb8a66e583b7"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Aligned eval samples: 683 / 683\n",
      "TinyLlama (text→z→volume): {'MAE': 793397.1208467651, 'RMSE': 1434803.3611238997, 'MAPE%': 37.41941244499241}\n",
      "Seasonal naive: {'MAE': 1035122.125, 'RMSE': 1966850.8488850903, 'MAPE%': 43.81184768676758}\n",
      "Moving average: {'MAE': 783198.1818787911, 'RMSE': 1475405.823909972, 'MAPE%': 32.859275970257514}\n",
      "00 | true=2165886.50  pred=2900389.06\n",
      "01 | true=3933762.75  pred=2886554.31\n",
      "02 | true=2430223.25  pred=3127971.16\n",
      "03 | true=3374707.00  pred=3127994.86\n",
      "04 | true=3410402.75  pred=2885001.76\n",
      "05 | true=3193953.75  pred=3127805.25\n",
      "06 | true=11314851.00  pred=2901927.90\n",
      "07 | true=5218058.50  pred=3376786.58\n",
      "08 | true=3779608.00  pred=3402445.45\n",
      "09 | true=4183481.25  pred=3402393.89\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Qwen 2.5 7B Training"
   ],
   "metadata": {
    "id": "56Qh9fbYS5HQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================  SETUP  ============================\n",
    "import os, re, math, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, LogitsProcessor\n",
    "from transformers.trainer_utils import set_seed\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from accelerate.utils import set_module_tensor_to_device\n",
    "\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# ============================  CONFIG  ============================\n",
    "CONFIG = {\n",
    "    \"feature_files\": [\"/content/features_trading_only_2.csv\"],\n",
    "    \"date_col\": \"date\",\n",
    "    \"vol_col\": \"volume\",\n",
    "    \"label_col\": \"z_target\",\n",
    "    \"context_len\": 64,\n",
    "    \"max_features\": 16,\n",
    "    \"model_name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"seed\": 42,\n",
    "    \"train_frac\": 0.9,\n",
    "    \"epochs\": 3,\n",
    "    \"lr\": 1e-4,\n",
    "    \"train_bs\": 1,\n",
    "    \"grad_accum\": 32,\n",
    "    \"max_length\": 2048,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"output_dir\": \"/content/qwen25_7b_ts_lora\",\n",
    "    \"bf16\": True,\n",
    "}\n",
    "set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "# ======================  LOAD & MERGE FEATURES  ==================\n",
    "def load_and_merge(paths: List[str], date_col: str):\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        df = pd.read_csv(p)\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[date_col])\n",
    "        dfs.append(df)\n",
    "    all_df = pd.concat(dfs, axis=0, ignore_index=True).sort_values(date_col).reset_index(drop=True)\n",
    "    return all_df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "\n",
    "df = load_and_merge(CONFIG[\"feature_files\"], CONFIG[\"date_col\"])\n",
    "\n",
    "# =====================  SELECT TOP-K FEATURES  ====================\n",
    "EXCL = {CONFIG[\"date_col\"], CONFIG[\"label_col\"], CONFIG[\"vol_col\"], \"y_trading\", \"y_log1p\"}\n",
    "num = df.select_dtypes(include=[np.number]).copy()\n",
    "std = num.std(numeric_only=True)\n",
    "non_const = std[std > 0].index.tolist()\n",
    "num = num[non_const]\n",
    "cand = [c for c in num.columns if c not in EXCL]\n",
    "corr = num[cand].corrwith(num[CONFIG[\"label_col\"]]).abs().replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "feature_cols = corr.sort_values(ascending=False).index.tolist()[:CONFIG[\"max_features\"]]\n",
    "\n",
    "\n",
    "vol = df[CONFIG[\"vol_col\"]].astype(float).values.reshape(-1,1)\n",
    "z_hist_scaler = StandardScaler()\n",
    "z_hist_series = z_hist_scaler.fit_transform(np.log1p(vol)).reshape(-1)\n",
    "\n",
    "def make_windows(df, z_hist, ctx, feat_cols, label_col):\n",
    "    X, Y = [], []\n",
    "    for t in range(ctx, len(df)):\n",
    "        X.append((z_hist[t-ctx:t].tolist(), df.iloc[t][feat_cols].to_dict()))\n",
    "        Y.append(float(df.iloc[t][label_col]))\n",
    "    return X, np.array(Y, dtype=np.float32)\n",
    "\n",
    "X_raw, Y = make_windows(df, z_hist_series, CONFIG[\"context_len\"], feature_cols, CONFIG[\"label_col\"])\n",
    "N = len(X_raw); cut = int(N * CONFIG[\"train_frac\"])\n",
    "\n",
    "def ex_to_text(hist, feats, y_z):\n",
    "    hs = \", \".join(f\"{x:.4f}\" for x in hist)\n",
    "    fs = \", \".join(f\"{k}={float(v):.4f}\" for k,v in feats.items()) if feats else \"none\"\n",
    "    return {\"prompt\": f\"z_hist[{len(hist)}]:{hs}\\nfeat:{fs}\\nnext_z:\", \"target\": f\"{y_z:.5f}\\n\"}\n",
    "\n",
    "train_text = [ex_to_text(*X_raw[i], Y[i]) for i in range(cut)]\n",
    "val_text   = [ex_to_text(*X_raw[i], Y[i]) for i in range(cut, N)]\n",
    "\n",
    "#   TOKENIZER\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"], use_fast=True, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dtype = torch.bfloat16 if (CONFIG[\"bf16\"] and torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8) else torch.float16\n",
    "\n",
    "OFFLOAD_DIR = os.path.join(CONFIG[\"output_dir\"], \"offload\")\n",
    "os.makedirs(OFFLOAD_DIR, exist_ok=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=False,\n",
    "    offload_state_dict=True,\n",
    "    offload_folder=OFFLOAD_DIR,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"], lora_alpha=CONFIG[\"lora_alpha\"], lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "target_device = next(model.parameters()).device\n",
    "for name, param in list(model.named_parameters()) + list(model.named_buffers()):\n",
    "    if hasattr(param, \"device\") and str(param.device) == \"meta\":\n",
    "        set_module_tensor_to_device(model, name, device=target_device)\n",
    "\n",
    "#   DATASET\n",
    "class TxtDS(Dataset):\n",
    "    def __init__(self, examples, tok, max_len=2048):\n",
    "        self.ex, self.tok, self.max_len = examples, tok, max_len\n",
    "    def __len__(self): return len(self.ex)\n",
    "    def __getitem__(self, i):\n",
    "        e = self.ex[i]\n",
    "        p_ids = self.tok(e[\"prompt\"], add_special_tokens=False)[\"input_ids\"]\n",
    "        t_ids = self.tok(e[\"target\"], add_special_tokens=False)[\"input_ids\"]\n",
    "        ids = p_ids + t_ids\n",
    "        if len(ids) > self.max_len:\n",
    "            overflow = len(ids) - self.max_len\n",
    "            keep_p = max(0, len(p_ids) - overflow)\n",
    "            ids = p_ids[-keep_p:] + t_ids\n",
    "        p_len = min(len(p_ids), len(ids) - len(t_ids))\n",
    "        labels = [-100]*p_len + ids[p_len:]\n",
    "        attn = [1]*len(ids)\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def pad_batch(batch, pad_id):\n",
    "    mx = max(len(b[\"input_ids\"]) for b in batch)\n",
    "    out = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    for b in batch:\n",
    "        pad_n = mx - len(b[\"input_ids\"])\n",
    "        out[\"input_ids\"].append(torch.cat([b[\"input_ids\"], torch.full((pad_n,), pad_id, dtype=torch.long)]))\n",
    "        out[\"attention_mask\"].append(torch.cat([b[\"attention_mask\"], torch.zeros(pad_n, dtype=torch.long)]))\n",
    "        out[\"labels\"].append(torch.cat([b[\"labels\"], torch.full((pad_n,), -100, dtype=torch.long)]))\n",
    "    return {k: torch.stack(v) for k,v in out.items()}\n",
    "\n",
    "train_ds = TxtDS(train_text, tokenizer, CONFIG[\"max_length\"])\n",
    "val_ds   = TxtDS(val_text,   tokenizer, CONFIG[\"max_length\"])\n",
    "\n",
    "#   TRAIN\n",
    "args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"train_bs\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"grad_accum\"],\n",
    "    learning_rate=CONFIG[\"lr\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    bf16=(dtype==torch.bfloat16),\n",
    "    fp16=(dtype==torch.float16),\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,  # not used during training; kept for consistency\n",
    "    data_collator=lambda b: pad_batch(b, tokenizer.pad_token_id),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "#  EVAL HELPERS\n",
    "class DigitsOnly(LogitsProcessor):\n",
    "    def __init__(self, tok, device):\n",
    "        allowed = set(\"0123456789-+.eE \\n\")\n",
    "        ids = []\n",
    "        for i in range(tok.vocab_size):\n",
    "            s = tok.decode([i])\n",
    "            if s and set(s).issubset(allowed): ids.append(i)\n",
    "        self.allowed_ids = torch.tensor(ids, device=device)\n",
    "    def __call__(self, input_ids, scores):\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        mask[:, self.allowed_ids] = 0\n",
    "        return scores + mask\n",
    "\n",
    "digits_only = DigitsOnly(tokenizer, device=next(model.parameters()).device)\n",
    "\n",
    "def number_from_text(s: str) -> Optional[float]:\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", s)\n",
    "    return float(m.group(0)) if m else None\n",
    "\n",
    "def load_scaler_json(paths: List[str]):\n",
    "    base = os.path.dirname(paths[0])\n",
    "    cand = os.path.join(base, \"features_trading_only_scaler_2.json\")\n",
    "    if os.path.exists(cand):\n",
    "        with open(cand, \"r\") as f:\n",
    "            s = json.load(f)\n",
    "        return float(s[\"y_log1p_mean\"]), float(s[\"y_log1p_std\"])\n",
    "    mu = float(df[\"y_log1p\"].mean()) if \"y_log1p\" in df.columns else 0.0\n",
    "    sigma = float(df[\"y_log1p\"].std(ddof=0)) if \"y_log1p\" in df.columns and df[\"y_log1p\"].std(ddof=0)>0 else 1.0\n",
    "    return mu, sigma\n",
    "\n",
    "mu, sigma = load_scaler_json(CONFIG[\"feature_files\"])\n",
    "\n",
    "def evaluate(model, tok, val_examples, mu, sigma, max_new_tokens=16):\n",
    "    model.eval()\n",
    "    preds_z, trues_z = [], []\n",
    "    for ex in val_examples:\n",
    "        ids = tok(ex[\"prompt\"], return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **ids, max_new_tokens=max_new_tokens, do_sample=False,\n",
    "                logits_processor=[digits_only],\n",
    "                pad_token_id=tok.pad_token_id, eos_token_id=tok.eos_token_id,\n",
    "            )\n",
    "        gen = tok.decode(out[0][ids[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        z_hat = number_from_text(gen)\n",
    "        if z_hat is None: continue\n",
    "        preds_z.append(z_hat)\n",
    "        # pull true z from the text target string\n",
    "        z_true = float(re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", ex[\"target\"])[0])\n",
    "        trues_z.append(z_true)\n",
    "    if not preds_z:\n",
    "        return {\"val_MAE\": float(\"nan\"), \"val_RMSE\": float(\"nan\")}\n",
    "    preds_z = np.array(preds_z); trues_z = np.array(trues_z)\n",
    "    y_pred = np.expm1(preds_z * sigma + mu)\n",
    "    y_true = np.expm1(trues_z * sigma + mu)\n",
    "    return {\n",
    "        \"val_MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"val_RMSE\": math.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "val_pairs = [{\"prompt\": e[\"prompt\"], \"target\": e[\"target\"]} for e in val_text]\n",
    "metrics = evaluate(model, tokenizer, val_pairs, mu, sigma)\n",
    "print(\"Validation metrics:\", metrics)\n",
    "\n",
    "# ===================  INFERENCE HELPER  ==========================\n",
    "def forecast_next(raw_recent_volumes: List[float], last_feat_row: Dict[str,float], mu: float, sigma: float, k_decimals=5) -> float:\n",
    "    z_hist = z_hist_scaler.transform(np.log1p(np.array(raw_recent_volumes).reshape(-1,1))).reshape(-1)\n",
    "    hist_str = \", \".join(f\"{z:.4f}\" for z in z_hist[-CONFIG[\"context_len\"]:])\n",
    "    feats_str = \", \".join(f\"{k}={float(v):.4f}\" for k,v in last_feat_row.items()) if last_feat_row else \"none\"\n",
    "    prompt = f\"z_hist[{len(z_hist[-CONFIG['context_len']:])}]:{hist_str}\\nfeat:{feats_str}\\nnext_z:\"\n",
    "    ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **ids, max_new_tokens=16, do_sample=False,\n",
    "            logits_processor=[digits_only],\n",
    "            pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    gen = tokenizer.decode(out[0][ids[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    z_hat = number_from_text(gen)\n",
    "    if z_hat is None:\n",
    "        raise RuntimeError(\"Model did not return a numeric answer.\")\n",
    "    y_hat = np.expm1(z_hat * sigma + mu)\n",
    "    return float(round(y_hat, k_decimals))\n",
    "\n",
    "print(\"Qwen2.5-7B LoRA: training done, metrics printed, forecast_next(...) ready.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "bea944c7dea94b5fadc7243cea37d014",
      "cec1ee24f0f14488977d378beef6d4a0",
      "5418e33854f64f9eb60e3513176f4bb2",
      "755380b2b862491bbcf5afdcea1914bb",
      "d62ed7af8ea444da90b63cdc754b6c73",
      "76c918e0bbf8489db8b97856897d46db",
      "688e5f8853dc4d28b21c8cc2d152579c",
      "5faa27cbd0a44beb99cbf15502d308bf",
      "de82bf8438a64810944488717c92dc1f",
      "9901286900ee49c6b3c6efb2b753ea74",
      "1928cdaa211f4b72b3a71845627d78b9"
     ]
    },
    "id": "3e6OF9pFI_qm",
    "outputId": "22faa8e2-6318-42cf-ca49-ded374db4785"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bea944c7dea94b5fadc7243cea37d014"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainable params: 40,370,176 || all params: 7,655,986,688 || trainable%: 0.5273\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-3451523866.py:181: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='576' max='576' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [576/576 1:39:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.963900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.436600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.435100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.435200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.423700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.432500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.419100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.421600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.421400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation metrics: {'val_MAE': 744230.5754974667, 'val_RMSE': 1416663.6189370814}\n",
      "Qwen2.5-7B LoRA: training done, metrics printed, forecast_next(...) ready.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Example of Bad Performance:\n",
    "\n",
    "I by mistake inferenced the fine-tuned Qwen, without considering the LoRA adapters and shooting up of Predictions happened"
   ],
   "metadata": {
    "id": "OY9RyjCiTPpS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "MODEL_PATH = \"/content/qwen25_7b_ts_lora\"\n",
    "\n",
    "import os, warnings, re, math, json\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "try:\n",
    "    from transformers.utils import logging as hf_logging\n",
    "    hf_logging.set_verbosity_error()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import numpy as np, pandas as pd, torch\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessor\n",
    "\n",
    "\n",
    "if \"model\" not in globals() or \"tokenizer\" not in globals():\n",
    "    if not MODEL_PATH:\n",
    "        raise RuntimeError(\"Set MODEL_PATH to your fine-tuned Qwen 2.5-7B checkpoint or load model/tokenizer beforehand.\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "torch.set_grad_enabled(False)\n",
    "model.eval()\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "FEATURE_FILE = \"/content/features_trading_only_2.csv\"\n",
    "DATE_COL, VOL_COL, LABEL_COL = \"date\", \"volume\", \"z_target\"\n",
    "CTX, MAX_FEATURES, TRAIN_FRAC, MAX_LEN = 64, 16, 0.9, 1024\n",
    "\n",
    "# ----------------- Data prep -----------------\n",
    "df = pd.read_csv(FEATURE_FILE)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[DATE_COL]).sort_values(DATE_COL).reset_index(drop=True)\n",
    "df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "\n",
    "# Top-K by |Pearson| vs label (exclude leakage)\n",
    "EXCL = {DATE_COL, LABEL_COL, VOL_COL, \"y_trading\", \"y_log1p\"}\n",
    "num = df.select_dtypes(include=[np.number]).copy()\n",
    "std = num.std(numeric_only=True)\n",
    "non_const = std[std > 0].index.tolist()\n",
    "num = num[non_const]\n",
    "cand = [c for c in num.columns if c not in EXCL and c != LABEL_COL]\n",
    "if not cand:\n",
    "    raise ValueError(\"No candidate numeric features found.\")\n",
    "corr = num[cand].corrwith(num[LABEL_COL]).abs().replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "feature_cols = corr.sort_values(ascending=False).index.tolist()[:MAX_FEATURES]\n",
    "\n",
    "# z-history from log1p(volume)\n",
    "vol = df[VOL_COL].astype(float).values.reshape(-1,1)\n",
    "z_hist_scaler = StandardScaler()\n",
    "z_hist_series = z_hist_scaler.fit_transform(np.log1p(vol)).reshape(-1)\n",
    "\n",
    "def make_windows(df, z_hist, ctx, feat_cols, label_col):\n",
    "    X, Y = [], []\n",
    "    for t in range(ctx, len(df)):\n",
    "        X.append((z_hist[t-ctx:t].tolist(), df.iloc[t][feat_cols].to_dict()))\n",
    "        Y.append(float(df.iloc[t][label_col]))\n",
    "    return X, np.array(Y, dtype=np.float32)\n",
    "\n",
    "X_raw, Y = make_windows(df, z_hist_series, CTX, feature_cols, LABEL_COL)\n",
    "cut = int(len(X_raw) * TRAIN_FRAC)\n",
    "val_text = []\n",
    "for hist, feats in X_raw[cut:]:\n",
    "    hist_str  = \", \".join(f\"{x:.4f}\" for x in hist)\n",
    "    feats_str = \", \".join(f\"{k}={float(v):.4f}\" for k,v in feats.items()) if feats else \"none\"\n",
    "    prompt = f\"z_hist[{len(hist)}]:{hist_str}\\nfeat:{feats_str}\\nnext_z:\"\n",
    "    val_text.append({\"prompt\": prompt})\n",
    "\n",
    "# ----------------- Scaler for inversion -----------------\n",
    "def load_scaler_json(feature_path: str):\n",
    "    base = os.path.dirname(feature_path)\n",
    "    cand = os.path.join(base, \"features_trading_only_scaler_2.json\")\n",
    "    if os.path.exists(cand):\n",
    "        try:\n",
    "            with open(cand, \"r\") as f:\n",
    "                s = json.load(f)\n",
    "            return float(s[\"y_log1p_mean\"]), float(s[\"y_log1p_std\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    if \"y_log1p\" in df.columns:\n",
    "        mu = float(df[\"y_log1p\"].mean())\n",
    "        sd = float(df[\"y_log1p\"].std(ddof=0))\n",
    "        return mu, (sd if sd and sd > 0 else 1.0)\n",
    "    return 0.0, 1.0\n",
    "\n",
    "mu, sigma = load_scaler_json(FEATURE_FILE)\n",
    "sigma = 1.0 if (not np.isfinite(sigma) or sigma == 0) else sigma\n",
    "\n",
    "# y_val (ground truth)\n",
    "y_va_z = Y[cut:]\n",
    "y_val  = np.expm1(y_va_z * sigma + mu)\n",
    "\n",
    "# Numeric-constrained decoding\n",
    "class DigitsOnly(LogitsProcessor):\n",
    "    def __init__(self, tok, device):\n",
    "        allowed = set(\"0123456789-+.eE \\n\")\n",
    "        ids = []\n",
    "        for i in range(tok.vocab_size):\n",
    "            try:\n",
    "                s = tok.decode([i])\n",
    "            except Exception:\n",
    "                continue\n",
    "            if s and set(s).issubset(allowed):\n",
    "                ids.append(i)\n",
    "        if not ids:\n",
    "            raise RuntimeError(\"DigitsOnly: no allowed token ids found for this tokenizer.\")\n",
    "        self.allowed_ids = torch.tensor(ids, device=device)\n",
    "    def __call__(self, input_ids, scores):\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        mask[:, self.allowed_ids] = 0\n",
    "        return scores + mask\n",
    "\n",
    "digits_only = DigitsOnly(tokenizer, device=device)\n",
    "num_pat = re.compile(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\")\n",
    "def number_from_text(s):\n",
    "    m = num_pat.search(s)\n",
    "    return float(m.group(0)) if m else np.nan\n",
    "\n",
    "# ----------------- Predict z -----------------\n",
    "preds_z = []\n",
    "for ex in val_text:\n",
    "    ids = tokenizer(ex[\"prompt\"], return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(device)\n",
    "    out = model.generate(\n",
    "        **ids,\n",
    "        max_new_tokens=12,\n",
    "        do_sample=False,\n",
    "        logits_processor=[digits_only],\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    gen = tokenizer.decode(out[0][ids[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    preds_z.append(number_from_text(gen))\n",
    "\n",
    "preds_z = np.array(preds_z, dtype=float)\n",
    "\n",
    "# ----------------- Overflow-safe inversion -----------------\n",
    "\n",
    "y_log1p_series = np.log1p(df[VOL_COL].astype(float).clip(lower=0))\n",
    "q_lo, q_hi = np.nanpercentile(y_log1p_series, [0.1, 99.9])\n",
    "margin = 0.5 * sigma\n",
    "y_log1p_min = max(-50.0, float(q_lo - margin))\n",
    "y_log1p_max = min( 50.0, float(q_hi + margin))\n",
    "z_min = (y_log1p_min - mu) / sigma\n",
    "z_max = (y_log1p_max - mu) / sigma\n",
    "\n",
    "mask = np.isfinite(preds_z)\n",
    "if mask.sum() == 0:\n",
    "    raise RuntimeError(\"Model returned no numeric outputs. Check prompts/decoding/template.\")\n",
    "z_hat = np.clip(preds_z[mask], z_min, z_max)\n",
    "\n",
    "y_pred = np.expm1(np.clip(z_hat * sigma + mu, y_log1p_min, y_log1p_max))\n",
    "y_true = y_val[mask]\n",
    "finite = np.isfinite(y_pred) & np.isfinite(y_true)\n",
    "y_pred = y_pred[finite]\n",
    "y_true = y_true[finite]\n",
    "\n",
    "# ----------------- Metrics -----------------\n",
    "def metrics(y_true, y_pred):\n",
    "    if len(y_true) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"MAPE%\": np.nan}\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / np.clip(y_true, 1e-9, None))) * 100)\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"MAPE%\": mape}\n",
    "\n",
    "print(f\"Aligned eval samples (finite): {len(y_true)} / {len(y_val)}\")\n",
    "print(\"Qwen2.5-7B (text→z→volume):\", metrics(y_true, y_pred))\n",
    "\n",
    "# Naive baselines on same span\n",
    "y_all = np.expm1(Y * sigma + mu)\n",
    "tail_len = len(y_true)\n",
    "truth_tail = y_all[-tail_len:]\n",
    "\n",
    "def seasonal_naive(series, season=5):\n",
    "    yhat = np.roll(series, season); yhat[:season] = series[:season]; return yhat\n",
    "def moving_avg(series, k=7):\n",
    "    s = pd.Series(series)\n",
    "    return s.rolling(k, min_periods=1).mean().shift(1).bfill().to_numpy()\n",
    "\n",
    "sn = seasonal_naive(np.r_[y_all[:-tail_len], truth_tail])[-tail_len:]\n",
    "ma = moving_avg(np.r_[y_all[:-tail_len], truth_tail])[-tail_len:]\n",
    "print(\"Seasonal naive:\", metrics(truth_tail, sn))\n",
    "print(\"Moving average:\", metrics(truth_tail, ma))\n",
    "\n",
    "# ----------------- Preview few predictions -----------------\n",
    "for i in range(min(10, len(y_true))):\n",
    "    print(f\"{i:02d} | true={y_true[i]:.2f}  pred={y_pred[i]:.2f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-oA7QO74HQo",
    "outputId": "6966f24a-d5a2-49db-86f8-0ec2c6aa8d6c"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Aligned eval samples (finite): 672 / 683\n",
      "Qwen2.5-7B (text→z→volume): {'MAE': 38030821.67282869, 'RMSE': 48829463.199295476, 'MAPE%': 2071.6677831837596}\n",
      "Seasonal naive: {'MAE': 1029828.875, 'RMSE': 1958129.1232275772, 'MAPE%': 44.14181137084961}\n",
      "Moving average: {'MAE': 773136.3305830145, 'RMSE': 1449779.1263772284, 'MAPE%': 32.98392204428371}\n",
      "00 | true=2165886.50  pred=69165194.28\n",
      "01 | true=3933762.75  pred=69165194.28\n",
      "02 | true=2430223.25  pred=69165194.28\n",
      "03 | true=3374707.00  pred=6734927.09\n",
      "04 | true=3410402.75  pred=3673372.34\n",
      "05 | true=3193953.75  pred=6734927.09\n",
      "06 | true=11314851.00  pred=69165194.28\n",
      "07 | true=5218058.50  pred=69165194.28\n",
      "08 | true=3779608.00  pred=13208479.05\n",
      "09 | true=4183481.25  pred=10611672.60\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finding and Loading LoRA adapters"
   ],
   "metadata": {
    "id": "_iChpF_gUxFe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ---- Auto-find and load the LoRA adapters\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "SEARCH_ROOTS = [\"/content/qwen25_7b_ts_lora\", \"/content\"]  # add more roots if needed\n",
    "device = \"cuda:0\"\n",
    "\n",
    "import os, time, torch, json\n",
    "from typing import Optional, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "try:\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    PEFT_OK = True\n",
    "except Exception:\n",
    "    PEFT_OK = False\n",
    "\n",
    "def find_latest_adapter(root_dirs) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Return dir containing BOTH adapter_model.* and adapter_config.json with the newest mtime.\n",
    "    Looks recursively under the given roots (handles trainer's checkpoint-* subdirs).\n",
    "    \"\"\"\n",
    "    best_dir, best_mtime = None, -1\n",
    "    for root in root_dirs:\n",
    "        if not os.path.exists(root):\n",
    "            continue\n",
    "        for cur, _, files in os.walk(root):\n",
    "            has_cfg = \"adapter_config.json\" in files\n",
    "            has_ad = any(f.startswith(\"adapter_model.\") for f in files)\n",
    "            if has_cfg and has_ad:\n",
    "                ad_files = [os.path.join(cur, f) for f in files if f.startswith(\"adapter_model.\")]\n",
    "                mtime = max(os.path.getmtime(p) for p in ad_files)\n",
    "                if mtime > best_mtime:\n",
    "                    best_mtime, best_dir = mtime, cur\n",
    "    return best_dir\n",
    "\n",
    "def find_merged_model(root_dirs) -> Optional[str]:\n",
    "    \"\"\"Return dir containing a full merged model (model.safetensors / pytorch_model.*).\"\"\"\n",
    "    for root in root_dirs:\n",
    "        if not os.path.exists(root):\n",
    "            continue\n",
    "        for cur, _, files in os.walk(root):\n",
    "            has_full = any(f in files for f in [\"model.safetensors\",\"pytorch_model.bin\",\"pytorch_model.safetensors\"])\n",
    "            if has_full:\n",
    "                return cur\n",
    "    return None\n",
    "\n",
    "def ensure_adapter_config(dirpath: str, base_model: str):\n",
    "    \"\"\"If adapter_config.json missing, write a minimal one that matches your LoRA training params.\"\"\"\n",
    "    cfg_path = os.path.join(dirpath, \"adapter_config.json\")\n",
    "    if os.path.exists(cfg_path):\n",
    "        return\n",
    "    lora_cfg = {\n",
    "        \"base_model_name_or_path\": base_model,\n",
    "        \"peft_type\": \"LORA\",\n",
    "        \"task_type\": \"CAUSAL_LM\",\n",
    "        \"r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\", \"inference_mode\": False,\n",
    "        \"target_modules\": [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    "    }\n",
    "    with open(cfg_path, \"w\") as f: json.dump(lora_cfg, f)\n",
    "    print(f\"[fix] wrote missing adapter_config.json at: {cfg_path}\")\n",
    "\n",
    "def load_ready_model_and_tokenizer(base_model: str, roots) -> Tuple[AutoModelForCausalLM, AutoTokenizer, str]:\n",
    "    # tokenizer ALWAYS from base\n",
    "    tok = AutoTokenizer.from_pretrained(base_model, use_fast=True, legacy=False)\n",
    "    if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "    dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8) else torch.float16\n",
    "\n",
    "    # 1) Try adapters (newest)\n",
    "    adir = find_latest_adapter(roots)\n",
    "    if adir and PEFT_OK:\n",
    "        ensure_adapter_config(adir, base_model)\n",
    "        try:\n",
    "            mdl = AutoPeftModelForCausalLM.from_pretrained(adir, torch_dtype=dtype, device_map=\"auto\")\n",
    "            mdl = mdl.merge_and_unload()\n",
    "            mdl.config.use_cache = False\n",
    "            print(f\"[ok] loaded & merged LoRA adapters from: {adir}\")\n",
    "            return mdl, tok, \"adapters_merged\"\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] adapter load failed at {adir}: {e}\")\n",
    "\n",
    "    # 2) Try full merged checkpoint\n",
    "    mdir = find_merged_model(roots)\n",
    "    if mdir:\n",
    "        mdl = AutoModelForCausalLM.from_pretrained(mdir, torch_dtype=dtype, device_map=\"auto\")\n",
    "        mdl.config.use_cache = False\n",
    "        print(f\"[ok] loaded full merged model from: {mdir}\")\n",
    "        return mdl, tok, \"merged_full\"\n",
    "\n",
    "    # 3) Fallback to base\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(base_model, torch_dtype=dtype, device_map=\"auto\")\n",
    "    mdl.config.use_cache = False\n",
    "    print(\"[WARN] adapters/merged not found — using BASE ONLY.\")\n",
    "    return mdl, tok, \"base_only\"\n",
    "\n",
    "model, tokenizer, mode = load_ready_model_and_tokenizer(BASE_MODEL, SEARCH_ROOTS)\n",
    "\n",
    "# Quick sanity print:\n",
    "print(\"mode:\", mode)\n",
    "print(\"device:\", next(model.parameters()).device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "fb8234991ccc4b7283e65413c5246487",
      "ca94d3d186f9496fba3799bd2b285e85",
      "78e51bfbfe2844fc9b20a77fd9fbb427",
      "e04a9c1aaa304adea24d437768e63356",
      "e810d031aafe4e8ea5503c89482c62f2",
      "20128f33e2da4d0a80aa9b401f930f7f",
      "df9e914ec52342e5b96f89f0be4c6943",
      "666136bdff0b4fcc91fb650afe1afe69",
      "b3d583e5872c451abd8e9aebaa894785",
      "c184a5fcbf994476b70da5bb0353da77",
      "f8f2d70d3ed646e6b3b85b279fa2abe5"
     ]
    },
    "id": "xo2VsfaFtw57",
    "outputId": "3da03964-7414-447b-f99b-c0d630df2a75"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb8234991ccc4b7283e65413c5246487"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ok] loaded & merged LoRA adapters from: /content/qwen25_7b_ts_lora/checkpoint-576\n",
      "mode: adapters_merged\n",
      "device: cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Performance of Qwen, when the LoRA adapters are taken into account"
   ],
   "metadata": {
    "id": "oFzgngD1TsO5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "MODEL_PATH = \"/content/qwen25_7b_ts_lora\"\n",
    "\n",
    "import os, warnings, re, math, json\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "try:\n",
    "    from transformers.utils import logging as hf_logging\n",
    "    hf_logging.set_verbosity_error()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import numpy as np, pandas as pd, torch\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessor\n",
    "\n",
    "# Load Qwen if not provided\n",
    "if \"model\" not in globals() or \"tokenizer\" not in globals():\n",
    "    if not MODEL_PATH:\n",
    "        raise RuntimeError(\"Set MODEL_PATH to your fine-tuned Qwen 2.5-7B checkpoint or load model/tokenizer beforehand.\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "# Ensure pad/eos are usable (Qwen often lacks pad_token by default)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "torch.set_grad_enabled(False)\n",
    "model.eval()\n",
    "\n",
    "# Config\n",
    "FEATURE_FILE = \"/content/features_trading_only_2.csv\"\n",
    "DATE_COL, VOL_COL, LABEL_COL = \"date\", \"volume\", \"z_target\"\n",
    "CTX, MAX_FEATURES, TRAIN_FRAC, MAX_LEN = 64, 16, 0.9, 1024\n",
    "\n",
    "# Data prep\n",
    "df = pd.read_csv(FEATURE_FILE)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[DATE_COL]).sort_values(DATE_COL).reset_index(drop=True)\n",
    "df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "\n",
    "# Top-K by |Pearson| vs label\n",
    "EXCL = {DATE_COL, LABEL_COL, VOL_COL, \"y_trading\", \"y_log1p\"}\n",
    "num = df.select_dtypes(include=[np.number]).copy()\n",
    "std = num.std(numeric_only=True)\n",
    "non_const = std[std > 0].index.tolist()\n",
    "num = num[non_const]\n",
    "cand = [c for c in num.columns if c not in EXCL and c != LABEL_COL]\n",
    "if not cand:\n",
    "    raise ValueError(\"No candidate numeric features found.\")\n",
    "corr = num[cand].corrwith(num[LABEL_COL]).abs().replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "feature_cols = corr.sort_values(ascending=False).index.tolist()[:MAX_FEATURES]\n",
    "\n",
    "# z-history from log1p(volume)\n",
    "vol = df[VOL_COL].astype(float).values.reshape(-1,1)\n",
    "z_hist_scaler = StandardScaler()\n",
    "z_hist_series = z_hist_scaler.fit_transform(np.log1p(vol)).reshape(-1)\n",
    "\n",
    "def make_windows(df, z_hist, ctx, feat_cols, label_col):\n",
    "    X, Y = [], []\n",
    "    for t in range(ctx, len(df)):\n",
    "        X.append((z_hist[t-ctx:t].tolist(), df.iloc[t][feat_cols].to_dict()))\n",
    "        Y.append(float(df.iloc[t][label_col]))\n",
    "    return X, np.array(Y, dtype=np.float32)\n",
    "\n",
    "X_raw, Y = make_windows(df, z_hist_series, CTX, feature_cols, LABEL_COL)\n",
    "cut = int(len(X_raw) * TRAIN_FRAC)\n",
    "val_text = []\n",
    "for hist, feats in X_raw[cut:]:\n",
    "    hist_str  = \", \".join(f\"{x:.4f}\" for x in hist)\n",
    "    feats_str = \", \".join(f\"{k}={float(v):.4f}\" for k,v in feats.items()) if feats else \"none\"\n",
    "    prompt = f\"z_hist[{len(hist)}]:{hist_str}\\nfeat:{feats_str}\\nnext_z:\"\n",
    "    val_text.append({\"prompt\": prompt})\n",
    "\n",
    "# Scaler for inversion\n",
    "def load_scaler_json(feature_path: str):\n",
    "    base = os.path.dirname(feature_path)\n",
    "    cand = os.path.join(base, \"features_trading_only_scaler_2.json\")\n",
    "    if os.path.exists(cand):\n",
    "        try:\n",
    "            with open(cand, \"r\") as f:\n",
    "                s = json.load(f)\n",
    "            return float(s[\"y_log1p_mean\"]), float(s[\"y_log1p_std\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    if \"y_log1p\" in df.columns:\n",
    "        mu = float(df[\"y_log1p\"].mean())\n",
    "        sd = float(df[\"y_log1p\"].std(ddof=0))\n",
    "        return mu, (sd if sd and sd > 0 else 1.0)\n",
    "    return 0.0, 1.0\n",
    "\n",
    "mu, sigma = load_scaler_json(FEATURE_FILE)\n",
    "sigma = 1.0 if (not np.isfinite(sigma) or sigma == 0) else sigma\n",
    "\n",
    "# y_val (ground truth)\n",
    "y_va_z = Y[cut:]\n",
    "y_val  = np.expm1(y_va_z * sigma + mu)\n",
    "\n",
    "\n",
    "class DigitsOnly(LogitsProcessor):\n",
    "    def __init__(self, tok, device):\n",
    "        allowed = set(\"0123456789-+.eE \\n\")\n",
    "        ids = []\n",
    "\n",
    "        for i in range(tok.vocab_size):\n",
    "            try:\n",
    "                s = tok.decode([i])\n",
    "            except Exception:\n",
    "\n",
    "                continue\n",
    "            if s and set(s).issubset(allowed):\n",
    "                ids.append(i)\n",
    "        if not ids:\n",
    "            raise RuntimeError(\"DigitsOnly: no allowed token ids found for this tokenizer.\")\n",
    "        self.allowed_ids = torch.tensor(ids, device=device)\n",
    "    def __call__(self, input_ids, scores):\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        mask[:, self.allowed_ids] = 0\n",
    "        return scores + mask\n",
    "\n",
    "digits_only = DigitsOnly(tokenizer, device=device)\n",
    "num_pat = re.compile(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\")\n",
    "def number_from_text(s):\n",
    "    m = num_pat.search(s)\n",
    "    return float(m.group(0)) if m else np.nan\n",
    "\n",
    "#  Predict z\n",
    "preds_z = []\n",
    "for ex in val_text:\n",
    "    ids = tokenizer(ex[\"prompt\"], return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(device)\n",
    "    out = model.generate(\n",
    "        **ids,\n",
    "        max_new_tokens=12,\n",
    "        do_sample=False,\n",
    "        logits_processor=[digits_only],\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    gen = tokenizer.decode(out[0][ids[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    preds_z.append(number_from_text(gen))\n",
    "\n",
    "preds_z = np.array(preds_z, dtype=float)\n",
    "\n",
    "# Overflow-safe inversion\n",
    "\n",
    "y_log1p_series = np.log1p(df[VOL_COL].astype(float).clip(lower=0))\n",
    "q_lo, q_hi = np.nanpercentile(y_log1p_series, [0.1, 99.9])\n",
    "margin = 0.5 * sigma\n",
    "y_log1p_min = max(-50.0, float(q_lo - margin))\n",
    "y_log1p_max = min( 50.0, float(q_hi + margin))\n",
    "z_min = (y_log1p_min - mu) / sigma\n",
    "z_max = (y_log1p_max - mu) / sigma\n",
    "\n",
    "mask = np.isfinite(preds_z)\n",
    "if mask.sum() == 0:\n",
    "    raise RuntimeError(\"Model returned no numeric outputs. Check prompts/decoding/template.\")\n",
    "z_hat = np.clip(preds_z[mask], z_min, z_max)\n",
    "\n",
    "y_pred = np.expm1(np.clip(z_hat * sigma + mu, y_log1p_min, y_log1p_max))\n",
    "y_true = y_val[mask]\n",
    "finite = np.isfinite(y_pred) & np.isfinite(y_true)\n",
    "y_pred = y_pred[finite]\n",
    "y_true = y_true[finite]\n",
    "\n",
    "#  Metrics\n",
    "def metrics(y_true, y_pred):\n",
    "    if len(y_true) == 0:\n",
    "        return {\"MAE\": np.nan, \"RMSE\": np.nan, \"MAPE%\": np.nan}\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / np.clip(y_true, 1e-9, None))) * 100)\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"MAPE%\": mape}\n",
    "\n",
    "print(f\"Aligned eval samples (finite): {len(y_true)} / {len(y_val)}\")\n",
    "print(\"Qwen2.5-7B (text→z→volume):\", metrics(y_true, y_pred))\n",
    "\n",
    "#  Naive baselines on same span\n",
    "y_all = np.expm1(Y * sigma + mu)\n",
    "tail_len = len(y_true)\n",
    "truth_tail = y_all[-tail_len:]\n",
    "\n",
    "def seasonal_naive(series, season=5):\n",
    "    yhat = np.roll(series, season); yhat[:season] = series[:season]; return yhat\n",
    "def moving_avg(series, k=7):\n",
    "    s = pd.Series(series)\n",
    "    return s.rolling(k, min_periods=1).mean().shift(1).bfill().to_numpy()\n",
    "\n",
    "sn = seasonal_naive(np.r_[y_all[:-tail_len], truth_tail])[-tail_len:]\n",
    "ma = moving_avg(np.r_[y_all[:-tail_len], truth_tail])[-tail_len:]\n",
    "print(\"Seasonal naive:\", metrics(truth_tail, sn))\n",
    "print(\"Moving average:\", metrics(truth_tail, ma))\n",
    "\n",
    "#  Preview few predictions\n",
    "for i in range(min(10, len(y_true))):\n",
    "    print(f\"{i:02d} | true={y_true[i]:.2f}  pred={y_pred[i]:.2f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "hvkeDxfN_2ue",
    "outputId": "9a39b0b3-d0fe-4564-966f-5e219e7fd95e"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Aligned eval samples (finite): 683 / 683\n",
      "Qwen2.5-7B (text→z→volume): {'MAE': 735187.8427327155, 'RMSE': 1409614.814201706, 'MAPE%': 32.24847408891829}\n",
      "Seasonal naive: {'MAE': 1035122.125, 'RMSE': 1966850.8488850903, 'MAPE%': 43.81184768676758}\n",
      "Moving average: {'MAE': 783198.1818787911, 'RMSE': 1475405.823909972, 'MAPE%': 32.859275970257514}\n",
      "00 | true=2165886.50  pred=3156519.44\n",
      "01 | true=3933762.75  pred=3645311.22\n",
      "02 | true=2430223.25  pred=3130555.75\n",
      "03 | true=3374707.00  pred=3645366.46\n",
      "04 | true=3410402.75  pred=3534069.10\n",
      "05 | true=3193953.75  pred=3670256.16\n",
      "06 | true=11314851.00  pred=3154391.42\n",
      "07 | true=5218058.50  pred=3130555.75\n",
      "08 | true=3779608.00  pred=4274142.85\n",
      "09 | true=4183481.25  pred=3642853.67\n"
     ]
    }
   ]
  }
 ]
}
