{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#Model(TinyLlama) training and evaluation"
   ],
   "metadata": {
    "id": "iufOpPLTamtU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training on Features_Group_1, Epochs 3"
   ],
   "metadata": {
    "id": "dGHT110natOn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import os, re, math, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import set_seed\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============================  CONFIG  ============================\n",
    "CONFIG = {\n",
    "    \"feature_files\": [\"/content/features_trading_only_2.csv\"],\n",
    "    \"date_col\": \"date\",\n",
    "    \"vol_col\": \"volume\",\n",
    "    \"label_col\": \"z_target\",\n",
    "    \"context_len\": 64,\n",
    "    \"max_features\": 16,\n",
    "    \"model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"seed\": 42,\n",
    "    \"train_frac\": 0.9,\n",
    "    \"epochs\": 3,\n",
    "    \"lr\": 1e-4,\n",
    "    \"train_bs\": 2,\n",
    "    \"grad_accum\": 16,\n",
    "    \"max_length\": 1024,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"output_dir\": \"/content/tinyllama_ts_lora\",\n",
    "    \"bf16\": True,\n",
    "}\n",
    "\n",
    "set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "# ======================  LOAD & MERGE FEATURES  ==================\n",
    "def load_and_merge(paths: List[str], date_col: str):\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        df = pd.read_csv(p)\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[date_col])\n",
    "        dfs.append(df)\n",
    "    all_df = pd.concat(dfs, axis=0, ignore_index=True).sort_values(date_col).reset_index(drop=True)\n",
    "    all_df = all_df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "    return all_df\n",
    "\n",
    "df = load_and_merge(CONFIG[\"feature_files\"], CONFIG[\"date_col\"])\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(df[[CONFIG[\"date_col\"], CONFIG[\"vol_col\"], \"y_trading\", \"y_log1p\", CONFIG[\"label_col\"]]].head())\n",
    "\n",
    "# =====================  SELECT TOP-K FEATURES  ====================\n",
    "EXCLUDE_COLS = {CONFIG[\"date_col\"], CONFIG[\"label_col\"], CONFIG[\"vol_col\"], \"y_trading\", \"y_log1p\"}\n",
    "num = df.select_dtypes(include=[np.number]).copy()\n",
    "std = num.std(numeric_only=True)\n",
    "non_const = std[std > 0].index.tolist()\n",
    "num = num[non_const]\n",
    "cand = [c for c in num.columns if c not in EXCLUDE_COLS]\n",
    "if not cand:\n",
    "    raise ValueError(\"No candidate numeric features found after exclusions.\")\n",
    "corr = num[cand].corrwith(num[CONFIG[\"label_col\"]]).abs().replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "feature_cols = corr.sort_values(ascending=False).index.tolist()[:CONFIG[\"max_features\"]]\n",
    "print(\"Selected feature columns:\", feature_cols)\n",
    "\n",
    "# ====================  BUILD z_history SERIES  ====================\n",
    "\n",
    "vol = df[CONFIG[\"vol_col\"]].astype(float).values.reshape(-1,1)\n",
    "z_hist_scaler = StandardScaler()\n",
    "z_hist_series = z_hist_scaler.fit_transform(np.log1p(vol)).reshape(-1)\n",
    "\n",
    "# ====================  BUILD WINDOWS  ====================\n",
    "def make_windows(df: pd.DataFrame, z_hist: np.ndarray, ctx: int, feat_cols: List[str], label_col: str):\n",
    "    X, Y = [], []\n",
    "    n = len(df)\n",
    "    for t in range(ctx, n):\n",
    "        hist = z_hist[t-ctx:t].tolist()\n",
    "        feats = df.iloc[t][feat_cols].to_dict()\n",
    "        y = float(df.iloc[t][label_col])\n",
    "        X.append((hist, feats))\n",
    "        Y.append(y)\n",
    "    return X, np.array(Y, dtype=np.float32)\n",
    "\n",
    "X_raw, Y = make_windows(df, z_hist_series, CONFIG[\"context_len\"], feature_cols, CONFIG[\"label_col\"])\n",
    "print(\"Total samples:\", len(X_raw))\n",
    "\n",
    "# ===================  TRAIN / VAL SPLIT  ==========================\n",
    "N = len(X_raw)\n",
    "cut = int(N * CONFIG[\"train_frac\"])\n",
    "train_idx = np.arange(0, cut)\n",
    "val_idx = np.arange(cut, N)\n",
    "\n",
    "def ex_to_text(hist, feats, y_z):\n",
    "    hist_str = \", \".join(f\"{x:.4f}\" for x in hist)\n",
    "    feats_str = \", \".join(f\"{k}={float(v):.4f}\" for k,v in feats.items()) if feats else \"none\"\n",
    "    prompt = f\"z_hist[{len(hist)}]:{hist_str}\\nfeat:{feats_str}\\nnext_z:\"\n",
    "    target = f\"{y_z:.5f}\\n\"\n",
    "    return {\"prompt\": prompt, \"target\": target}\n",
    "\n",
    "train_text = [ex_to_text(*X_raw[i], Y[i]) for i in train_idx]\n",
    "val_text   = [ex_to_text(*X_raw[i], Y[i]) for i in val_idx]\n",
    "\n",
    "# ===================  TOKENIZER / MODEL  =========================\n",
    "model_name = CONFIG[\"model_name\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "torch_dtype = (\n",
    "    torch.bfloat16\n",
    "    if (CONFIG[\"bf16\"] and torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8)\n",
    "    else torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",   # no extra installs\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# LoRA targets for TinyLlama blocks\n",
    "lora_cfg = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"], lora_alpha=CONFIG[\"lora_alpha\"], lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ===================  DATASET / COLLATOR  ========================\n",
    "class TxtDS(Dataset):\n",
    "    def __init__(self, examples, tok, max_len=1024):\n",
    "        self.ex = examples; self.tok = tok; self.max_len = max_len\n",
    "    def __len__(self): return len(self.ex)\n",
    "    def __getitem__(self, i):\n",
    "        e = self.ex[i]\n",
    "        p_ids = self.tok(e[\"prompt\"], add_special_tokens=False)[\"input_ids\"]\n",
    "        t_ids = self.tok(e[\"target\"], add_special_tokens=False)[\"input_ids\"]\n",
    "        ids = p_ids + t_ids\n",
    "        if len(ids) > self.max_len:\n",
    "            overflow = len(ids) - self.max_len\n",
    "            keep_p = max(0, len(p_ids) - overflow)\n",
    "            ids = p_ids[-keep_p:] + t_ids\n",
    "        p_len = min(len(p_ids), len(ids) - len(t_ids))\n",
    "        labels = [-100]*p_len + ids[p_len:]\n",
    "        attn = [1]*len(ids)\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def pad_batch(batch, pad_id):\n",
    "    mx = max(len(b[\"input_ids\"]) for b in batch)\n",
    "    out = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    for b in batch:\n",
    "        pad_n = mx - len(b[\"input_ids\"])\n",
    "        out[\"input_ids\"].append(torch.cat([b[\"input_ids\"], torch.full((pad_n,), pad_id, dtype=torch.long)]))\n",
    "        out[\"attention_mask\"].append(torch.cat([b[\"attention_mask\"], torch.zeros(pad_n, dtype=torch.long)]))\n",
    "        out[\"labels\"].append(torch.cat([b[\"labels\"], torch.full((pad_n,), -100, dtype=torch.long)]))\n",
    "    return {k: torch.stack(v) for k,v in out.items()}\n",
    "\n",
    "def collate_fn(features):\n",
    "    return pad_batch(features, tokenizer.pad_token_id)\n",
    "\n",
    "train_ds = TxtDS(train_text, tokenizer, CONFIG[\"max_length\"])\n",
    "val_ds   = TxtDS(val_text, tokenizer, CONFIG[\"max_length\"])\n",
    "\n",
    "# ===================  TRAIN (fast)  ==========================\n",
    "args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"train_bs\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"train_bs\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"grad_accum\"],\n",
    "    learning_rate=CONFIG[\"lr\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=(torch_dtype==torch.bfloat16),\n",
    "    fp16=(torch_dtype==torch.float16),\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=\"none\",\n",
    "    do_eval=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# ===================  EVAL (invert to original units)  ===========\n",
    "def load_scaler_json(paths):\n",
    "    base = os.path.dirname(paths[0])\n",
    "    cand = os.path.join(base, \"features_trading_only_scaler_2.json\")\n",
    "    if os.path.exists(cand):\n",
    "        with open(cand, \"r\") as f:\n",
    "            s = json.load(f)\n",
    "        return float(s[\"y_log1p_mean\"]), float(s[\"y_log1p_std\"])\n",
    "    mu = float(df[\"y_log1p\"].mean()) if \"y_log1p\" in df.columns else 0.0\n",
    "    sigma = float(df[\"y_log1p\"].std(ddof=0)) if \"y_log1p\" in df.columns and df[\"y_log1p\"].std(ddof=0)>0 else 1.0\n",
    "    return mu, sigma\n",
    "\n",
    "mu, sigma = load_scaler_json(CONFIG[\"feature_files\"])\n",
    "\n",
    "def number_from_text(text: str) -> Optional[float]:\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", text)\n",
    "    return float(m.group(0)) if m else None\n",
    "\n",
    "def evaluate(model, tok, val_examples, mu, sigma, max_new_tokens=12):\n",
    "    model.eval()\n",
    "    preds_z, trues_z = [], []\n",
    "    for ex in val_examples:\n",
    "        ids = tok(ex[\"prompt\"], return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **ids, max_new_tokens=max_new_tokens, do_sample=False,\n",
    "                pad_token_id=tok.pad_token_id, eos_token_id=tok.eos_token_id\n",
    "            )\n",
    "        gen = tok.decode(out[0][ids[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        z_hat = number_from_text(gen)\n",
    "        if z_hat is None: continue\n",
    "        preds_z.append(z_hat)\n",
    "        trues_z.append(float(re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", ex[\"target\"])[0]))\n",
    "    if not preds_z:\n",
    "        return {\"val_MAE\": float(\"nan\"), \"val_RMSE\": float(\"nan\")}\n",
    "    preds_z = np.array(preds_z); trues_z = np.array(trues_z)\n",
    "    y_pred = np.expm1(preds_z * sigma + mu)\n",
    "    y_true = np.expm1(trues_z * sigma + mu)\n",
    "    return {\n",
    "        \"val_MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"val_RMSE\": math.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "val_pairs = [{\"prompt\": e[\"prompt\"], \"target\": e[\"target\"]} for e in val_text]\n",
    "metrics = evaluate(model, tokenizer, val_pairs, mu, sigma)\n",
    "print(\"Validation metrics:\", metrics)\n",
    "\n",
    "# ===================  INFERENCE  ==========================\n",
    "def forecast_next(raw_recent_volumes: List[float], last_feat_row: Dict[str,float], mu: float, sigma: float, k_decimals=5) -> float:\n",
    "    z_hist = z_hist_scaler.transform(np.log1p(np.array(raw_recent_volumes).reshape(-1,1))).reshape(-1)\n",
    "    hist_str = \", \".join(f\"{z:.4f}\" for z in z_hist[-CONFIG[\"context_len\"]:])\n",
    "    feats_str = \", \".join(f\"{k}={float(v):.4f}\" for k,v in last_feat_row.items()) if last_feat_row else \"none\"\n",
    "    prompt = f\"z_hist[{len(z_hist[-CONFIG['context_len']:])}]:{hist_str}\\nfeat:{feats_str}\\nnext_z:\"\n",
    "    ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**ids, max_new_tokens=12, do_sample=False,\n",
    "                             pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    gen = tokenizer.decode(out[0][ids[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    z_hat = number_from_text(gen)\n",
    "    if z_hat is None: raise RuntimeError(\"Model did not return a numeric answer.\")\n",
    "    return float(round(np.expm1(z_hat * sigma + mu), k_decimals))\n",
    "\n",
    "print(\"TinyLlama LoRA fine-tune done. Use forecast_next(...) for inference.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "9cbb106c691d483c903ddbada516bea4",
      "ce3de2c105d64c0393c2028218e18106",
      "8d45bd8e1a8d426eaf7c3ff3789ace13",
      "9bd5a372fdd64b07bc4b94ef4052194e",
      "dce52531ad6a497dac5efd5d42b8abf3",
      "6d6f3b5f832746f38f19eb4ebf19911b",
      "bc28a949a7784f0f9240e6ff4e698e17",
      "c48859d343f147c28468df9ead6351d4",
      "56ddcf62c7a74fb89622e2e607499ed4",
      "9f138df21c6444d19361948bf268d6a7",
      "1ce21c8139124a1ab032d399d5979c8c",
      "d1e845458b454c49881e634ab596ea14",
      "0affffe20e3f49e9b315a0ffe0b6d705",
      "5417c2b84afc43cd83173489912b1a1f",
      "effdf006394c4374bb863c9d6fd0b784",
      "4e88106d2afb43b2bebb91dcb9b1a9d1",
      "f549969414a94353ba7482ecd6ee9a41",
      "0e0c658410544ad68bd44da7e3ee1af7",
      "347651a15c3c4e31b20b92ff3b23868f",
      "9e182dc6d3d74c598245e7b17452acc4",
      "0bfdb16b157c4ee9b8818a34c6b96e49",
      "53fa33d2427947a88abfe80c48a17fb3",
      "de18d7c71fe448e9b41cd4761b91f548",
      "f02fd258c629494390f34d47531517c5",
      "24323ef3496a424286e0452b1edb9162",
      "526ab95a895e4fc6a6b2baa13d0f2685",
      "2811d4f673fe40c68ea4a1f43be2a8b3",
      "464c5d1348be462d91ed959d2fd64221",
      "2fc62f3e31fd4b41a572a370d66d2f98",
      "a86010d8016540f58ff32da63a84efde",
      "30905ee36a6e44d2be8d151008110caf",
      "bdbf29d4ecb945b5a97ae4cc12a67f7a",
      "3591f0e5e5f042b8a05fa122ef23857d",
      "a0edbe3fa97b496985febce85e97c04e",
      "c628d9e3f9bb4dd4bf5e07738ffbe3d3",
      "49fbf2b1ba89491b886cd25e47ce6536",
      "dc895027551e4dcebc4cb1a3c1869fbf",
      "5622366f3f924ae984876622dbbf888a",
      "898fd60c38fb4f6bb3d86c9328fd398e",
      "ec6e9d2db7af4663974ba505ea0b1ee8",
      "4aa7a6b361d5408cb76ccc4ea42de566",
      "4654c7ccad3d4845ab21a960d630eb9e",
      "763ae356d6e9426bacee762f721c67c0",
      "59a379e3462f4f0dafce4656902ccb00",
      "1fed533dd1a04732b86c94b0ad31cf1e",
      "6244370f0c3b4954b74e4873c016557e",
      "3e563f2c03fc42f488ccd280a795be11",
      "4b5cfc801cc5425cb45bc31ccaf737db",
      "4a46cda1dc734b799d3423b0d4ad05a2",
      "da53a6974f1f4181b94f69eae8e534fd",
      "17097f754fc94c07a307fe2424cea290",
      "00f98924e7c04c88935daf8429786d30",
      "9f4edfd566dd4c6d842077623e6df2a3",
      "5493554cb54d4d5c8b534db051447c3d",
      "a234403bb88746dd83fb0add6f228e2a",
      "95b4a5d086e24b4784f1566bae72d72a",
      "df945ef9831b4ddcb2e36d20a8b7389e",
      "cac25423d1db4af1be22b04932270334",
      "7728c1cc76e34637ae3262f1af17d6d9",
      "1de1ef2f43b646238f676efdc98d4824",
      "fd5736c41ee4437e923d751f6eb4026d",
      "3957509909a547a38dbcb739d4329f6d",
      "d2acccc4192a4cb690a7ef17c035c452",
      "b3685d4c536b4fecbfd53e2ec040a509",
      "fb24212f77ad47a29fd88a73dfc2110b",
      "579f953766fb49d7a2f8fa7402540576",
      "9f7db269cedd4677b9e250b529e9f74a",
      "decc003e3b8a4df8b3914fbe7ff0bf01",
      "e73c46719d4d4393ad221ec4a59ccd3b",
      "e23c9e1dcd504ee1bb60c97144c43ff2",
      "f4a8ad7e67b04c9a9395907ba38509a3",
      "42c7e393e7614b9ab739757d0b393911",
      "cf88daf34dba4f85a5b5586e7a39c6a4",
      "8d62d8975a844a9897c76685a08220b1",
      "3c64398991dc41eaa76c4e10fca57576",
      "d53981cf3f8b4f4abd20a752c58a7745",
      "ebbf74711e6c46ccbe0a8d8d4cfa05f5"
     ]
    },
    "id": "0ZUkKJkj5trA",
    "outputId": "760cee78-22b1-4730-ec06-fae3c4618210"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data shape: (6890, 53)\n",
      "        date    volume   y_trading    y_log1p  z_target\n",
      "0 1998-02-11  15819189  13524081.0  16.419983  0.920051\n",
      "1 1998-02-12  13524081   8694402.0  15.978190  0.337016\n",
      "2 1998-02-13   8694402  14912102.0  16.517684  1.048988\n",
      "3 1998-02-17  14912102  12788824.0  16.364082  0.846279\n",
      "4 1998-02-18  12788824  16986901.0  16.647953  1.220905\n",
      "Selected feature columns: ['ema28', 'ema7', 'roll14_mean', 'roll14_sum', 'roll28_sum', 'roll28_mean', 'roll7_mean', 'roll7_sum', 'year', 'roll7_min', 'roll14_min', 'roll28_min', 'lag1', 'lag2', 'roll7_max', 'fourier_t_sin1']\n",
      "Total samples: 6826\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9cbb106c691d483c903ddbada516bea4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1e845458b454c49881e634ab596ea14"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de18d7c71fe448e9b41cd4761b91f548"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0edbe3fa97b496985febce85e97c04e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fed533dd1a04732b86c94b0ad31cf1e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95b4a5d086e24b4784f1566bae72d72a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f7db269cedd4677b9e250b529e9f74a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-930866006.py:201: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='576' max='576' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [576/576 41:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.722200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.348100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.341900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.340200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.335300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.344300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.337100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.327400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.333700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation metrics: {'val_MAE': 772539.2447184883, 'val_RMSE': 1432829.1655742198}\n",
      "TinyLlama LoRA fine-tune done. Use forecast_next(...) for inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading LoRA Adapters for Evaluation of Model"
   ],
   "metadata": {
    "id": "w6WbbsA3bZC5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ---- Auto-find and load your LoRA adapters ----\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "SEARCH_ROOTS = [\"/content/tinyllama_ts_lora\", \"/content\"]\n",
    "\n",
    "import os, time, torch, json\n",
    "from typing import Optional, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "try:\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    PEFT_OK = True\n",
    "except Exception:\n",
    "    PEFT_OK = False\n",
    "\n",
    "def find_latest_adapter(root_dirs) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Return dir containing BOTH adapter_model.* and adapter_config.json with the newest mtime.\n",
    "    Looks recursively under the given roots (handles trainer's checkpoint-* subdirs).\n",
    "    \"\"\"\n",
    "    best_dir, best_mtime = None, -1\n",
    "    for root in root_dirs:\n",
    "        if not os.path.exists(root):\n",
    "            continue\n",
    "        for cur, _, files in os.walk(root):\n",
    "            has_cfg = \"adapter_config.json\" in files\n",
    "            has_ad = any(f.startswith(\"adapter_model.\") for f in files)\n",
    "            if has_cfg and has_ad:\n",
    "                ad_files = [os.path.join(cur, f) for f in files if f.startswith(\"adapter_model.\")]\n",
    "                mtime = max(os.path.getmtime(p) for p in ad_files)\n",
    "                if mtime > best_mtime:\n",
    "                    best_mtime, best_dir = mtime, cur\n",
    "    return best_dir\n",
    "\n",
    "def find_merged_model(root_dirs) -> Optional[str]:\n",
    "    \"\"\"Return dir containing a full merged model (model.safetensors / pytorch_model.*).\"\"\"\n",
    "    for root in root_dirs:\n",
    "        if not os.path.exists(root):\n",
    "            continue\n",
    "        for cur, _, files in os.walk(root):\n",
    "            has_full = any(f in files for f in [\"model.safetensors\",\"pytorch_model.bin\",\"pytorch_model.safetensors\"])\n",
    "            if has_full:\n",
    "                return cur\n",
    "    return None\n",
    "\n",
    "def ensure_adapter_config(dirpath: str, base_model: str):\n",
    "    \"\"\"If adapter_config.json missing, write a minimal one that matches your LoRA training params.\"\"\"\n",
    "    cfg_path = os.path.join(dirpath, \"adapter_config.json\")\n",
    "    if os.path.exists(cfg_path):\n",
    "        return\n",
    "    lora_cfg = {\n",
    "        \"base_model_name_or_path\": base_model,\n",
    "        \"peft_type\": \"LORA\",\n",
    "        \"task_type\": \"CAUSAL_LM\",\n",
    "        \"r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\", \"inference_mode\": False,\n",
    "        \"target_modules\": [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    "    }\n",
    "    with open(cfg_path, \"w\") as f: json.dump(lora_cfg, f)\n",
    "    print(f\"[fix] wrote missing adapter_config.json at: {cfg_path}\")\n",
    "\n",
    "def load_ready_model_and_tokenizer(base_model: str, roots) -> Tuple[AutoModelForCausalLM, AutoTokenizer, str]:\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(base_model, use_fast=True, legacy=False)\n",
    "    if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "    dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8) else torch.float16\n",
    "\n",
    "\n",
    "    adir = find_latest_adapter(roots)\n",
    "    if adir and PEFT_OK:\n",
    "        ensure_adapter_config(adir, base_model)\n",
    "        try:\n",
    "            mdl = AutoPeftModelForCausalLM.from_pretrained(adir, torch_dtype=dtype, device_map=\"auto\")\n",
    "            mdl = mdl.merge_and_unload()\n",
    "            mdl.config.use_cache = False\n",
    "            print(f\"[ok] loaded & merged LoRA adapters from: {adir}\")\n",
    "            return mdl, tok, \"adapters_merged\"\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] adapter load failed at {adir}: {e}\")\n",
    "\n",
    "\n",
    "    mdir = find_merged_model(roots)\n",
    "    if mdir:\n",
    "        mdl = AutoModelForCausalLM.from_pretrained(mdir, torch_dtype=dtype, device_map=\"auto\")\n",
    "        mdl.config.use_cache = False\n",
    "        print(f\"[ok] loaded full merged model from: {mdir}\")\n",
    "        return mdl, tok, \"merged_full\"\n",
    "\n",
    "\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(base_model, torch_dtype=dtype, device_map=\"auto\")\n",
    "    mdl.config.use_cache = False\n",
    "    print(\"[WARN] adapters/merged not found — using BASE ONLY.\")\n",
    "    return mdl, tok, \"base_only\"\n",
    "\n",
    "model, tokenizer, mode = load_ready_model_and_tokenizer(BASE_MODEL, SEARCH_ROOTS)\n",
    "\n",
    "# sanity cheching:\n",
    "print(\"mode:\", mode)\n",
    "print(\"device:\", next(model.parameters()).device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oO_-G8FSXvy8",
    "outputId": "0fab082b-63cc-490b-b1c9-37dc460b8d6b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ok] loaded & merged LoRA adapters from: /content/tinyllama_ts_lora/checkpoint-576\n",
      "mode: adapters_merged\n",
      "device: cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Performance of TinyLlama, trained on Feature_Group_1 with 3 epochs!"
   ],
   "metadata": {
    "id": "PUAHAJ33bd--"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Config\n",
    "FEATURE_FILE = \"/content/features_trading_only_2.csv\"\n",
    "DATE_COL, VOL_COL, LABEL_COL = \"date\", \"volume\", \"z_target\"\n",
    "CTX, MAX_FEATURES, TRAIN_FRAC, MAX_LEN = 64, 16, 0.9, 1024\n",
    "\n",
    "import os, re, math, json, numpy as np, pandas as pd, torch\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from transformers import LogitsProcessor\n",
    "\n",
    "# ---------- Load & prep features ----------\n",
    "df = pd.read_csv(FEATURE_FILE)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[DATE_COL]).sort_values(DATE_COL).reset_index(drop=True)\n",
    "df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "\n",
    "# Top-K by absolute Pearson (keep consistent with your training)\n",
    "EXCL = {DATE_COL, LABEL_COL, VOL_COL, \"y_trading\", \"y_log1p\"}\n",
    "num = df.select_dtypes(include=[np.number]).copy()\n",
    "std = num.std(numeric_only=True)\n",
    "non_const = std[std > 0].index.tolist()\n",
    "num = num[non_const]\n",
    "cand = [c for c in num.columns if c not in EXCL]\n",
    "if not cand: raise ValueError(\"No candidate numeric features found.\")\n",
    "corr = num[cand].corrwith(num[LABEL_COL]).abs().replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "feature_cols = corr.sort_values(ascending=False).index.tolist()[:MAX_FEATURES]\n",
    "\n",
    "# z-history from log1p(volume)\n",
    "vol = df[VOL_COL].astype(float).values.reshape(-1,1)\n",
    "z_hist_scaler = StandardScaler()\n",
    "z_hist_series = z_hist_scaler.fit_transform(np.log1p(vol)).reshape(-1)\n",
    "\n",
    "def make_windows(df, z_hist, ctx, feat_cols, label_col):\n",
    "    X, Y = [], []\n",
    "    for t in range(ctx, len(df)):\n",
    "        X.append((z_hist[t-ctx:t].tolist(), df.iloc[t][feat_cols].to_dict()))\n",
    "        Y.append(float(df.iloc[t][label_col]))\n",
    "    return X, np.array(Y, dtype=np.float32)\n",
    "\n",
    "X_raw, Y = make_windows(df, z_hist_series, CTX, feature_cols, LABEL_COL)\n",
    "cut = int(len(X_raw) * TRAIN_FRAC)\n",
    "val_text = []\n",
    "for hist, feats in X_raw[cut:]:\n",
    "    hist_str  = \", \".join(f\"{x:.4f}\" for x in hist)\n",
    "    feats_str = \", \".join(f\"{k}={float(v):.4f}\" for k,v in feats.items()) if feats else \"none\"\n",
    "    prompt = f\"z_hist[{len(hist)}]:{hist_str}\\nfeat:{feats_str}\\nnext_z:\"\n",
    "    val_text.append({\"prompt\": prompt})\n",
    "\n",
    "# scaler to invert to original units\n",
    "def load_scaler_json(feature_path: str):\n",
    "    base = os.path.dirname(feature_path)\n",
    "    cand = os.path.join(base, \"features_trading_only_scaler_2.json\")\n",
    "    if os.path.exists(cand):\n",
    "        with open(cand, \"r\") as f:\n",
    "            s = json.load(f)\n",
    "        return float(s[\"y_log1p_mean\"]), float(s[\"y_log1p_std\"])\n",
    "    mu = float(df[\"y_log1p\"].mean()) if \"y_log1p\" in df.columns else 0.0\n",
    "    std = df[\"y_log1p\"].std(ddof=0) if \"y_log1p\" in df.columns else 1.0\n",
    "    return mu, (float(std) if std and std > 0 else 1.0)\n",
    "\n",
    "mu, sigma = load_scaler_json(FEATURE_FILE)\n",
    "\n",
    "# y_val (ground truth)\n",
    "y_va_z = Y[cut:]\n",
    "y_val  = np.expm1(y_va_z * sigma + mu)\n",
    "\n",
    "# ---------- Numeric-constrained decoding ----------\n",
    "class DigitsOnly(LogitsProcessor):\n",
    "    def __init__(self, tok, device):\n",
    "        allowed_chars = set(\"0123456789-+.eE \\n\")\n",
    "        ids = []\n",
    "        for i in range(tok.vocab_size):\n",
    "            s = tok.decode([i])\n",
    "            if s and set(s).issubset(allowed_chars):\n",
    "                ids.append(i)\n",
    "        self.allowed_ids = torch.tensor(ids, device=device)\n",
    "    def __call__(self, input_ids, scores):\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        mask[:, self.allowed_ids] = 0\n",
    "        return scores + mask\n",
    "\n",
    "digits_only = DigitsOnly(tokenizer, device=next(model.parameters()).device)\n",
    "num_pat = re.compile(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\")\n",
    "def number_from_text(s):\n",
    "    m = num_pat.search(s);\n",
    "    return float(m.group(0)) if m else None\n",
    "\n",
    "# ---------- Predict TinyLlama on validation ----------\n",
    "model.eval()\n",
    "preds_z = []\n",
    "for ex in val_text:\n",
    "    ids = tokenizer(ex[\"prompt\"], return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(next(model.parameters()).device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**ids, max_new_tokens=12, do_sample=False,\n",
    "                             logits_processor=[digits_only],\n",
    "                             pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    gen = tokenizer.decode(out[0][ids[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    z_hat = number_from_text(gen)\n",
    "    preds_z.append(np.nan if z_hat is None else z_hat)\n",
    "\n",
    "preds_z = np.array(preds_z, dtype=float)\n",
    "mask = ~np.isnan(preds_z)\n",
    "if mask.sum() == 0:\n",
    "    raise RuntimeError(\"Model returned no numeric outputs. Check prompts/decoding.\")\n",
    "y_pred = np.expm1(preds_z[mask] * sigma + mu)\n",
    "y_true = y_val[mask]\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "def metrics(y_true, y_pred):\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / np.clip(y_true, 1e-9, None))) * 100)\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE%\": mape}\n",
    "\n",
    "print(f\"Aligned eval samples: {len(y_true)} / {len(y_val)}\")\n",
    "print(\"TinyLlama (text→z→volume):\", metrics(y_true, y_pred))\n",
    "\n",
    "# ---------- Naive baselines on same span ----------\n",
    "\n",
    "y_all = np.expm1(Y * sigma + mu)\n",
    "tail_len = len(y_true)\n",
    "truth_tail = y_all[-tail_len:]\n",
    "\n",
    "def seasonal_naive(series, season=5):\n",
    "    yhat = np.roll(series, season); yhat[:season] = series[:season]; return yhat\n",
    "def moving_avg(series, k=7):\n",
    "    s = pd.Series(series)\n",
    "    return s.rolling(k, min_periods=1).mean().shift(1).bfill().to_numpy()\n",
    "\n",
    "sn = seasonal_naive(np.r_[y_all[:-tail_len], truth_tail])[-tail_len:]\n",
    "ma = moving_avg(np.r_[y_all[:-tail_len], truth_tail])[-tail_len:]\n",
    "print(\"Seasonal naive:\", metrics(truth_tail, sn))\n",
    "print(\"Moving average:\", metrics(truth_tail, ma))\n",
    "\n",
    "# ---------- Preview few predictions ----------\n",
    "for i in range(min(10, len(y_true))):\n",
    "    print(f\"{i:02d} | true={y_true[i]:.2f}  pred={y_pred[i]:.2f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9CIxeYUZb6p",
    "outputId": "98b551b4-d2a0-4f1e-bbde-7bd2bd25c5e6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Aligned eval samples: 683 / 683\n",
      "TinyLlama (text→z→volume): {'MAE': 769220.187365776, 'RMSE': 1419827.828691292, 'MAPE%': 35.59028644550582}\n",
      "Seasonal naive: {'MAE': 1035122.125, 'RMSE': 1966850.8488850903, 'MAPE%': 43.81184768676758}\n",
      "Moving average: {'MAE': 783198.1818787911, 'RMSE': 1475405.823909972, 'MAPE%': 32.859275970257514}\n",
      "00 | true=2165886.50  pred=2899729.81\n",
      "01 | true=3933762.75  pred=3013983.48\n",
      "02 | true=2430223.25  pred=2679824.59\n",
      "03 | true=3374707.00  pred=2901927.90\n",
      "04 | true=3410402.75  pred=2749887.98\n",
      "05 | true=3193953.75  pred=3130365.99\n",
      "06 | true=11314851.00  pred=3106547.08\n",
      "07 | true=5218058.50  pred=3376760.99\n",
      "08 | true=3779608.00  pred=3376581.88\n",
      "09 | true=4183481.25  pred=3615025.68\n"
     ]
    }
   ]
  }
 ]
}
