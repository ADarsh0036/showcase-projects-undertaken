{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKFv1QBxR16-"
   },
   "outputs": [],
   "source": [
    "#House Price Prediction\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#print(sys.argv)\n",
    "\n",
    "data=pd.read_csv(r\"./housepriceprediction/housing_California_Housing_Data.csv\")#delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsWdX6jpM3VE",
    "outputId": "1859a963-caac-4738-90e0-c6b8e383e180"
   },
   "outputs": [],
   "source": [
    "\"\"\"from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "mpP05rdLxQiV",
    "outputId": "1731a721-7edf-42f2-9cbd-f3fb72bc7ab8"
   },
   "outputs": [],
   "source": [
    "data.head()\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qsT_pi0nywK8",
    "outputId": "b98888b3-25dc-4c69-c8e0-04d8c6993d97"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bV0N_lzBNtxG"
   },
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9phJf38qOXN0",
    "outputId": "b049fda6-fd96-4ada-bc9e-f17b4b794d9b"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gSMpPb1aali"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyF6I3Ffaalj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4qz3Yyqaalj"
   },
   "outputs": [],
   "source": [
    "X = data.drop(['median_house_value'], axis=1)\n",
    "y = data['median_house_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36MHnm7LYycG"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.feature_selection import RFE\n",
    "\n",
    "X=data.drop([\"median_house_value\"],axis=1)\n",
    "Y=data[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "9-vVt0rVZxPo",
    "outputId": "1169b46e-88d1-42c7-f75d-54637ff14b7a"
   },
   "outputs": [],
   "source": [
    "print(X.shape);\n",
    "X #,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FxcF6F_aaln"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "stscaler = StandardScaler()\n",
    "\n",
    "#print(X.shape)\n",
    "#Xnew = X.drop(columns=['ocean_proximity']) #pd.get_dummies(data,columns=['ocean_proximity'])\n",
    "#Xnew = scaler.fit_transform(X)\n",
    "#print(Xnew.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "eIxQe-F3UAvh",
    "outputId": "4cd61532-2217-490b-aeaa-cd85ca1f009d"
   },
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPneeIz1awy9"
   },
   "outputs": [],
   "source": [
    "#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "KZxHEOYxbcvv",
    "outputId": "4915744a-d015-4c59-a141-bf87c284a1d4"
   },
   "outputs": [],
   "source": [
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgP-R1h5aalr",
    "outputId": "4821b8ec-d58e-478d-8fcc-10f68d7783fb"
   },
   "outputs": [],
   "source": [
    "X_ = X.drop(['ocean_proximity'],axis=1)\n",
    "Xnew = scaler.fit_transform(X_)\n",
    "print(Xnew.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9lkZblnbjyL"
   },
   "outputs": [],
   "source": [
    "d_train=X.join(Y)\n",
    "#this drops all columns of  ocean proximity and appends the household value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff06Eeu6aalt"
   },
   "source": [
    "### Dataset: features and targets\n",
    "\n",
    "Target is Median_house_value and there are 9 features with \"ocean_proximity\" having different class of string values. We plot a heatmap of the covariance matrix to determine the important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "XR9p-Q5_bwZp",
    "outputId": "813d5691-d97f-43da-9ec7-e79487740fa6"
   },
   "outputs": [],
   "source": [
    "d_train.sort_values(by=['median_house_value'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "id": "u5CcK6wzb-Ry",
    "outputId": "7f41ad34-af0f-43a2-fe0a-911424c909c5"
   },
   "outputs": [],
   "source": [
    "d_train.hist(figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "apo27Wffddq8",
    "outputId": "8c2c0071-01b3-44f3-f466-d21abf10027a"
   },
   "outputs": [],
   "source": [
    "d_train.corr()\n",
    "d_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "jx0HC4CXdisI",
    "outputId": "df68514a-b23f-4044-de4a-b0c94e3e9913"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "sns.heatmap(d_train.corr(),annot=True,cmap=\"YlGnBu\")\n",
    "#Seeing The Heat Map It Can Be Noted That, The Price Of the House Is Very Much Correlated With The Area and The Number Of Bathroom\"\n",
    "#The Price Of The House IS Correlated With The Following In The Decreasing Order->Area->Number Of Bathrooms->Number Of Parking->Number Of Stories->Number Of Bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2zZzTypialy"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "d_train[\"total_rooms\"]=np.log(d_train[\"total_rooms\"]+1)\n",
    "d_train[\"total_bedrooms\"]=np.log(d_train[\"total_bedrooms\"]+1)\n",
    "d_train[\"population\"]=np.log(d_train[\"population\"]+1)\n",
    "d_train[\"households\"]=np.log(d_train[\"households\"]+1)\n",
    "#d_train[\"bedrooms\"]=np.log(d_train[\"bedrooms\"]+1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8EaOIWOibRR"
   },
   "outputs": [],
   "source": [
    "#d_train[\"ocean_proximity\"].value_counts()\n",
    "#Returns a Series containing counts of unique rows in the DataFrame.\n",
    "#We sssign feature Of {0,1} for each condition like semifurninished, unfurnished and furnished.\n",
    "#Basically a one-hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1c6by0HEjs4W"
   },
   "outputs": [],
   "source": [
    "#pd.get_dummies(d_train[\"ocean_proximity\"])\n",
    "#Converts categorical variable into dummy/indicator variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "c4Byg-idlbxn",
    "outputId": "7bb20d13-ba3d-4142-eee1-eccdc4ce3fcd"
   },
   "outputs": [],
   "source": [
    "#Already done, need not be repeated again if d_train.shape == (20433, 14/16)\n",
    "#d_train=d_train.join(pd.get_dummies(d_train.ocean_proximity)).drop([\"ocean_proximity\",\"median_house_value\"],axis=1).join(data[\"median_house_value\"])\n",
    "#d_train\n",
    "print(d_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "aFP12e-qlzyn",
    "outputId": "7b2d341d-ad5e-43bb-cdde-6578397f2d9d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "sns.scatterplot(x=\"longitude\",y=\"latitude\",data=d_train,hue=\"median_house_value\",palette=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "ygQoZm1gn34N",
    "outputId": "7e68b84e-bce6-41a6-d0e5-21ab20452d5b"
   },
   "outputs": [],
   "source": [
    "d_train[\"bedroom ratio\"]=d_train[\"total_bedrooms\"]/d_train[\"total_rooms\"]\n",
    "d_train[\"household rooms\"]=d_train[\"total_rooms\"]/d_train[\"households\"]\n",
    "d_train\n",
    "d_train.drop([\"median_house_value\"],axis=1).join(data[\"median_house_value\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "DQLH6w2Rt520",
    "outputId": "e2fe0e5b-e4bd-43c7-d882-d9dde153d6f5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "sns.heatmap(d_train.corr(),annot=True,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xf5FFNAUrpf"
   },
   "source": [
    "## Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Psxe5TEYaal_"
   },
   "source": [
    "Having looked at the confusion/covariance matrix, we find the following features to be most important in predicting the Household value:\n",
    "* Median_income\n",
    "* < 1 hr Proximity to the ocean\n",
    "* Total number of rooms\n",
    "\n",
    "whereas following features have a noticeable negative correlation to the Household value:\n",
    "* being Inland, i.e. far away from the ocean\n",
    "* bedroom to total house-area ratio\n",
    "\n",
    "We start with the vanilla gradient descent algorithm and check the prediction efficiency of the linear regression estimator w.r.t. the dominant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5RsvrQ9aal_"
   },
   "source": [
    "### Gradients of a Mean-square Linear regression estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJl_AMjgaamA"
   },
   "source": [
    "$$ J = \\frac{1}{2m} (\\hat{y} - w \\cdot X^T - b)^2 $$\n",
    "$$ \\partial J/\\partial w = -\\frac{1}{2m} X^T (\\hat{y} - w \\cdot X^T - b)$$\n",
    "$$ \\partial J/\\partial b = -\\frac{1}{2m} (\\hat{y} - w \\cdot X^T - b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jP4mSDwaamA",
    "outputId": "c2da237d-34dd-4098-c1fb-8d7191ef72df"
   },
   "outputs": [],
   "source": [
    "feature = d_train.drop(['median_house_value'], axis=1)\n",
    "label = d_train.median_house_value\n",
    "print(feature.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGXdHqctaamA"
   },
   "outputs": [],
   "source": [
    "featureT = stscaler.fit_transform(feature.values)\n",
    "labelT = stscaler.fit_transform(d_train.median_house_value.values.reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltRPL9MJaamB",
    "outputId": "7bb66eef-8f22-40af-e89c-6cc418dfa9de"
   },
   "outputs": [],
   "source": [
    "print(featureT.shape)\n",
    "featureT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8B4Un8vsaamC",
    "outputId": "4d796757-107c-4ce1-dbf7-135d99189ba1"
   },
   "outputs": [],
   "source": [
    "print(labelT.shape)\n",
    "labelT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbgI9svMaamC",
    "outputId": "53250b3e-8b6f-4205-baeb-cad41b4d8410"
   },
   "outputs": [],
   "source": [
    "\"\"\"import numpy as np\n",
    "\n",
    "# Generate some random data for demonstration purposes\n",
    "n, d = d_train.shape   # Number of data points, # Number of features\n",
    "X = featureT  # n data points with d features\n",
    "y = labelT  # n data points with 1 output variable\n",
    "\n",
    "\n",
    "# Initialize weights and biases\n",
    "# here the initial values of the W- and b-column vectors will be fed\n",
    "pickstate = np.random.randint(0,n)\n",
    "w = featureT[pickstate]\n",
    "b = labelT[pickstate]\n",
    "print(pickstate)\n",
    "print(w.shape)\n",
    "print(b)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxNO_eotaamD",
    "outputId": "fddc4bf3-368f-4e7f-8826-26560a6a0007"
   },
   "outputs": [],
   "source": [
    "#w = np.random.randn(d, 1)  # d weights for the d features\n",
    "#b = np.random.randn()  # Bias\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Generate some random data for demonstration purposes\n",
    "n, d = d_train.shape   # Number of data points, # Number of features\n",
    "X = featureT  # n data points with d features\n",
    "y = labelT  # n data points with 1 output variable\n",
    "\n",
    "\n",
    "# Initialize weights and biases\n",
    "# here the initial values of the W- and b-column vectors will be fed\n",
    "pickstate = np.random.randint(0,n)\n",
    "w = featureT[pickstate]\n",
    "b = labelT[pickstate]\n",
    "print(pickstate)\n",
    "print(w.shape)\n",
    "print(b)\n",
    "\n",
    "# Set hyperparameters\n",
    "\n",
    "\n",
    "alpha = 0.1 # Learning rate\n",
    "tolerance = 0.001\n",
    "\n",
    "num_iterations = 1000  # Number of iterations\n",
    "CEAS1 = np.zeros((3, num_iterations))\n",
    "CEAS2 = np.zeros((3, num_iterations))\n",
    "\n",
    "\n",
    "\n",
    "# Perform gradient descent\n",
    "#def gradD(alpha,w,b):\n",
    "cost_at_each_step = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Compute predicted values\n",
    "    y_hat = np.dot(X, w) + b\n",
    "\n",
    "    # Compute error\n",
    "    error = y_hat - y\n",
    "\n",
    "    # Compute gradients\n",
    "    dw = np.dot(X.T, error) / n\n",
    "    db = np.sum(error) / n\n",
    "\n",
    "    # Update weights and biases\n",
    "    w -= alpha * dw\n",
    "    b -= alpha * db\n",
    "\n",
    "    # Compute cost for monitoring convergence\n",
    "    cost = np.mean(error**2)\n",
    "\n",
    "    cost_at_each_step.append(cost)\n",
    "    #if i>=1 and (cost - cost_at_each_step[i-1] <= tolerance):\n",
    "    #    print(\"Error tolerance touched in {} steps\".format(i))\n",
    "    #    break\n",
    "    # Print cost every 100 iterations\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration\", i, \"Cost:\", cost)\n",
    "    #return cost_at_each_step\n",
    "\n",
    "# Make predictions on new data\n",
    "X_new = np.random.randn(10, d-1)  # 10 new data points with d features\n",
    "y_pred = np.dot(X_new, w) + b\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n",
    "\"\"\"\n",
    "tol = 0.001\n",
    "for ai,alph in enumerate(Alpha):\n",
    "    CEAS1[ai] = gradD(alph,w,b)\n",
    "\"\"\"\n",
    "#print(cost_at_each_step)\n",
    "zz = np.diff(cost_at_each_step)\n",
    "print(np.where(np.abs(zz) <= tol))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCjeAecZaamP"
   },
   "outputs": [],
   "source": [
    "CEAS1[0] = cost_at_each_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUrwj7fSaamQ"
   },
   "outputs": [],
   "source": [
    "CEAS1[1] = cost_at_each_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cetoDMhBaamQ"
   },
   "outputs": [],
   "source": [
    "CEAS1[2] = cost_at_each_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRw-dvBWaamR",
    "outputId": "af051e22-9596-4dc4-e71f-304257654cb1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "Alpha = np.array([0.01, 0.05, 0.1])\n",
    "\n",
    "for k in range(3):\n",
    "    plt.semilogy(np.arange(num_iterations)[:320], CEAS1[k,:320]) #, label =f'alpha = {Alpha[k]}')\n",
    "\n",
    "#plt.legend()\n",
    "plt.xlabel('iteration count')\n",
    "plt.ylabel('Loss value')\n",
    "plt.title('Convergence for vanilla gradient descent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViIqax97aamR"
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "\n",
    "def nesterov_accelerated_gradient_descent(X, y, learning_rate, momentum, num_iterations):\n",
    "    # Initialize the weight vector\n",
    "    w = np.zeros(X.shape[1])\n",
    "    # Initialize the bias term\n",
    "    b = 0\n",
    "    # Initialize the velocity vector\n",
    "    v = np.zeros(X.shape[1])\n",
    "    # Initialize the cost history list\n",
    "    cost_history = []\n",
    "    # Loop through the specified number of iterations\n",
    "    for i in range(num_iterations):\n",
    "        # Update the velocity using the previous weight and momentum\n",
    "        v = momentum * v - learning_rate * np.dot(X, w - momentum * v) / X.shape[0]\n",
    "        # Update the weight using the velocity\n",
    "        w += v\n",
    "        # Make predictions using the current weight vector and bias term\n",
    "        y_pred = np.dot(X, w) + b\n",
    "        # Calculate the cost (mean squared error)\n",
    "        cost = np.mean((y_pred - y) ** 2)\n",
    "        # Append the cost to the cost history list\n",
    "        cost_history.append(cost)\n",
    "        # Print cost every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            print(\"Iteration\", i, \"Cost:\", cost)\n",
    "    return w, b, cost_history\n",
    "nesterov_accelerated_gradient_descent(X, y, learning_rate, momentum, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kRl3Q3POZ2RP",
    "outputId": "5cd20396-02df-487c-b405-e59ade09bf7b"
   },
   "outputs": [],
   "source": [
    "# W=np.zeros((np.shape(data_X1)[0],np.shape(data_X1)[1]))\n",
    "# W\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        #print('shape of y=',np.shape(y))\n",
    "\n",
    "        #initialize weights and bias to zeros\n",
    "        self.weights=np.zeros((n_features,1))\n",
    "        #print(np.shape(self.weights))\n",
    "\n",
    "   #gradient descent\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "          #computing predictions and errors\n",
    "          y_pred=np.dot(X,self.weights)+self.bias\n",
    "          error=y_pred-y\n",
    "\n",
    "          #compute gradients\n",
    "\n",
    "          delw=(1/n_samples)*np.dot(X.T,error)\n",
    "          #print(np.shape(delw))\n",
    "          delb=(1/n_samples)*np.sum(error)\n",
    "         # print(np.shape(delb))\n",
    "\n",
    "          #update weights and bias\n",
    "\n",
    "          self.weights=self.weights-self.learning_rate*delw\n",
    "          self.bias=self.bias-self.learning_rate*delb\n",
    "\n",
    "    def predict(self,X):\n",
    "      y_pred=np.dot(X,self.weights)+self.bias\n",
    "      return y_pred\n",
    "\n",
    "  #Creating The Linear Regression Object\n",
    "lr=LinearRegression()\n",
    "\n",
    "#generate some example data\n",
    "\n",
    "# X=np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "# y=np.array([4.8,8.3,11.8])\n",
    "\n",
    "#fit the model to the data\n",
    "\n",
    "#X1 = d_train[\"median_income\"]\n",
    "#X2 = d_train[\"<1H OCEAN\"]\n",
    "#X1.head()\n",
    "\n",
    "\n",
    "lr.fit(Xf1,Yt1)\n",
    "\n",
    "#make a prediction on new data\n",
    "y_pred=lr.predict(data_Xts1)\n",
    "# X_new=np.array([[2,3,4],[5,6,7]])\n",
    "# y_pred=lr.predict(X_new)\n",
    "\n",
    "print(y_pred)\n",
    "# print(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JYk3JxSaamT"
   },
   "source": [
    "### Convergence analysis\n",
    "To add a convergence criteria to the gradient descent algorithm, we can keep track of the difference between successive predictions in each iteration. We can stop the algorithm if this difference falls below the desired tolerance limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wmhabsk-aamU"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train,Y_train=d_train.drop([\"median_house_value\"],axis=1),d_train[\"median_house_value\"]\n",
    "reg=LinearRegression()\n",
    "reg.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "thMkDe3LHBAJ",
    "outputId": "f3fc9c89-9cdd-45e6-95bc-96076e4fb8db"
   },
   "outputs": [],
   "source": [
    "d_test=X_test.join(Y_test)\n",
    "#d_test=d_test.drop([\"bedroom_ratio\",\"household_rooms\"],axis=1)\n",
    "d_test[\"bedroom ratio\"]=d_test[\"total_bedrooms\"]/d_test[\"total_rooms\"]\n",
    "d_test[\"household rooms\"]=d_test[\"total_rooms\"]/d_test[\"households\"]\n",
    "d_test\n",
    "\n",
    "# temp_data=d_test.drop([\"median_house_value\",\"household rooms\",\"bedroom ratio\",\"ISLAND\",\"NEAR OCEAN\",\"NEAR BAY\"],axis=1)\n",
    "# temp_data\n",
    "# temp_data=temp_data.join(d_test[\"ISLAND\"])\n",
    "# temp_data=temp_data.join(d_test[\"NEAR BAY\"])\n",
    "# temp_data=temp_data.join(d_test[\"NEAR OCEAN\"])\n",
    "# temp_data=temp_data.join(d_test[\"bedroom ratio\"])\n",
    "# temp_data=temp_data.join(d_test[\"household rooms\"])\n",
    "# temp_data=temp_data.join(d_test[\"median_house_value\"])\n",
    "\n",
    "# d_test=temp_data\n",
    "d_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyVVMfK6HZ4m"
   },
   "outputs": [],
   "source": [
    "X_test,Y_test=d_test.drop([\"median_house_value\"],axis=1),d_test[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mcYIo4WEHpiQ",
    "outputId": "f076ad9a-6762-4883-c0c4-122f29039e48"
   },
   "outputs": [],
   "source": [
    "reg.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSsyfsLNTKxo"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Define the dataset\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2, 4, 5, 4, 5]\n",
    "\n",
    "# Initialize the parameters\n",
    "theta = 0  # the parameter, including the intercept\n",
    "\n",
    "# Set the learning rate\n",
    "alpha = 0.01\n",
    "\n",
    "# Define the tolerance limit\n",
    "tolerance = 0.01\n",
    "\n",
    "# Define the number of iterations\n",
    "num_iters = 1000\n",
    "\n",
    "# Implement the gradient descent algorithm\n",
    "diff = float('inf')  # initialize the difference between successive predictions\n",
    "i = 0  # initialize the number of iterations\n",
    "while diff > tolerance and i < num_iters:\n",
    "    h = theta * x  # the predicted values\n",
    "    error = h - y  # the errors\n",
    "    grad = x * error  # the gradient\n",
    "    theta -= alpha * grad  # update the parameter\n",
    "    new_h = theta * x  # the new predicted values\n",
    "    diff = abs(new_h - h)  # the difference between successive predictions\n",
    "    h = new_h\n",
    "    i += 1\n",
    "\n",
    "# Print the final value of the parameter and the number of iterations\n",
    "print('theta:', theta)\n",
    "print('iterations:', i)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfpGQSWIaamX"
   },
   "source": [
    "The velocity of the parameter update is initialized as v, and use it to modify the gradient update rule. The momentum parameter controls how much of the previous velocity to include in the current update. The rest of the code is the same as before, with the convergence criteria and the number of iterations being checked in the while loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SV1njntaamX"
   },
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2, 4, 5, 4, 5]\n",
    "\n",
    "# Initialize the parameters\n",
    "theta = 0  # the parameter, including the intercept\n",
    "v = 0  # the velocity of the parameter update\n",
    "\n",
    "# Set the learning rate\n",
    "alpha = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "# Define the tolerance limit\n",
    "tolerance = 0.01\n",
    "\n",
    "# Define the number of iterations\n",
    "num_iters = 1000\n",
    "\n",
    "# Implement the Nesterov accelerated gradient descent algorithm\n",
    "diff = float('inf')  # initialize the difference between successive predictions\n",
    "i = 0  # initialize the number of iterations\n",
    "while diff > tolerance and i < num_iters:\n",
    "    h = (theta - momentum * v) * x  # the predicted values\n",
    "    error = h - y  # the errors\n",
    "    grad = x * error  # the gradient\n",
    "    v = momentum * v - alpha * grad  # update the velocity\n",
    "    theta += -momentum * v + (1 + momentum) * alpha * grad  # update the parameter\n",
    "    new_h = (theta - momentum * v) * x  # the new predicted values\n",
    "    diff = abs(new_h - h)  # the difference between successive predictions\n",
    "    h = new_h\n",
    "    i += 1\n",
    "\n",
    "# Print the final value of the parameter and the number of iterations\n",
    "print('theta:', theta)\n",
    "print('iterations:', i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eY6aKmZdaamY"
   },
   "outputs": [],
   "source": [
    "plt.contourf(theta_mesh, cost_mesh, np.log(cost_mesh), cmap=plt.cm.viridis)\n",
    "plt.colorbar()\n",
    "plt.plot(*zip(*path), color='red')\n",
    "plt.xlabel('theta')\n",
    "plt.ylabel('cost')\n",
    "plt.title('Gradient Descent Iterations')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
