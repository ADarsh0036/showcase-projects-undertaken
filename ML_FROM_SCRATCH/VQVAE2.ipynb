{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, hidden_channels = 256, wheretouse = \"encoder\"):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        if wheretouse == \"encoder\":\n",
    "            self.resblk = nn.Sequential(\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(hidden_channels, hidden_channels, 3, 1, 1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(hidden_channels, hidden_channels, 1, 1, 0)\n",
    "            )\n",
    "        elif wheretouse == \"decoder\":\n",
    "            self.resblk = nn.Sequential(\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(hidden_channels, hidden_channels, 1, 1, 0),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(hidden_channels, hidden_channels, 3, 1, 1)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return torch.relu(x + self.resblk(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ResBlock()\n",
    "outres = res(torch.randn(2, 256, 8, 8))\n",
    "outres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, latent_dim, num_residual_layers = 2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels // 2, 4, 2, 1))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_channels // 2, hidden_channels, 4, 2, 1))\n",
    "        self.num_residual_layers = num_residual_layers\n",
    "        self.resblk = nn.Sequential(*[ResBlock(wheretouse = \"encoder\") for _ in range(num_residual_layers)])\n",
    "        self.conv_latent = nn.Conv2d(hidden_channels, latent_dim, 1, 1, 0)  #Just that the output of the residual blocks now gets mapped to the latent space.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.resblk(x)\n",
    "        ze_x = self.conv_latent(x)\n",
    "        \n",
    "        return ze_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(3, 256, 64, 2)\n",
    "out = encoder(torch.randn(2, 3, 32, 32))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_channels, latent_dim, hidden_channels, num_residual_layers = 2):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.convdec = nn.Conv2d(latent_dim, hidden_channels, 1, 1, 0)    # Just to project from latent dimension to the hidden_channel dimension\n",
    "        self.resblkdec = nn.Sequential(*[ResBlock(wheretouse = \"decoder\") for _ in range(num_residual_layers)])\n",
    "        \n",
    "        self.convT1 = nn.Sequential(nn.ConvTranspose2d(hidden_channels, hidden_channels // 2, 4, 2, 1), nn.ReLU())\n",
    "        self.convT2 = nn.ConvTranspose2d(hidden_channels // 2, out_channels, 4, 2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.convdec(x)\n",
    "        x = self.resblkdec(x)\n",
    "        x = self.convT1(x)\n",
    "        x_cap = self.convT2(x)\n",
    "        \n",
    "        return x_cap\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(3, 64, 256)\n",
    "out3 = decoder(out)\n",
    "out3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, beta, epsilon = 1e-10):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        \n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # self.decay = decay\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        embedding = torch.Tensor(self.num_embeddings, self.embedding_dim)\n",
    "        init_bound = 1/self.num_embeddings\n",
    "        embedding.uniform_(-init_bound, init_bound)\n",
    "        self.register_buffer(\"embedding\", embedding)\n",
    "        self.register_buffer(\"ema_count\", torch.zeros(self.num_embeddings))\n",
    "        self.register_buffer(\"ema_weight\", self.embedding.clone())\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        K, D = self.embedding.size()\n",
    "        x_flat = x.detach().reshape(-1, D)\n",
    "        \n",
    "        distances = (-torch.cdist(x_flat, self.embedding, p = 2)) ** 2   #outputs a matrix of size(x_flat.size()[0], K), with each entry being the distance between x_flat \n",
    "        \n",
    "        indices = torch.argmin(distances.float(), dim = -1)    #A (x_flat.size[0], 1) shaped vector, whose each entry give the index of the latent vector, to which corresponding x_flat vector has the minimum distance from it.\n",
    "        encoding = F.one_hot(indices, K).float()\n",
    "        quantized = F.embedding(indices, self.embedding)    # selects the embedding vector corresponding to index from range {1, 2, 3, ..., K}\n",
    "        # quantized_mod = quantized.view_as(x)\n",
    "        \n",
    "        \n",
    "        # codebook_loss = F.mse_loss(x.detach(), quantized_mod)   #why not x_flat and quantized\n",
    "        # commitment_loss = self.beta*F.mse_loss(x, quantized_mod.detach())\n",
    "        codebook_loss = F.mse_loss(x_flat.detach(), quantized)\n",
    "        commitment_loss = self.beta*F.mse_loss(x_flat, quantized.detach())\n",
    "        \n",
    "        # quantized_mod = x + (quantized_mod - x).detach()  #while forward quantized  = quantized, while backward pass, quantized = x why??\n",
    "        quantized = x_flat + (quantized - x_flat).detach()\n",
    "        \n",
    "        avg_probs = torch.mean(encoding, dim = 0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + self.epsilon)))\n",
    "        \n",
    "        return quantized, codebook_loss, commitment_loss, perplexity\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorQuantizer(512, 64, 0.25)\n",
    "out2 = vq(out)\n",
    "out2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, latent_dim, num_embeddings, embedding_dim, beta, num_residual_layers = 2):\n",
    "        super(VQVAE, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(in_channels, hidden_channels, latent_dim)\n",
    "        self.decoder = Decoder(in_channels,latent_dim, hidden_channels)\n",
    "        self.vectorquantizer = VectorQuantizer(num_embeddings, embedding_dim, beta)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ze_x = self.encoder(x)\n",
    "        zq_x, codebook_loss, commitment_loss, perplexity = self.vectorquantizer(ze_x)\n",
    "        zq_x = zq_x.view_as(ze_x)\n",
    "        x_cap = self.decoder(zq_x)\n",
    "        \n",
    "        return x_cap, codebook_loss, commitment_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "IN_CHANNELS = 3\n",
    "HIDDEN_CHANNELS = 256\n",
    "LATENT_DIM = 64\n",
    "NUM_EMBEDDINGS = 512\n",
    "EMBEDDING_DIM = 64\n",
    "BETA = 0.25\n",
    "lr = 2e-4\n",
    "\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "vqvae = VQVAE(in_channels = IN_CHANNELS, hidden_channels = HIDDEN_CHANNELS, latent_dim = LATENT_DIM, num_embeddings = NUM_EMBEDDINGS, embedding_dim = EMBEDDING_DIM, beta = BETA).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.CIFAR10(root = \"/mnt/External/8TBHDD/AdityaAshokBarot\", train = True, transform = transform, download = True)\n",
    "val_dataset = datasets.CIFAR10(root = \"/mnt/External/8TBHDD/AdityaAshokBarot\", train = False, transform = transform, download = True)\n",
    "\n",
    "kwargs = {\"pin_memory\": True, \"num_workers\":4}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, **kwargs)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = True, **kwargs)\n",
    "\n",
    "print_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vqvae.parameters(), lr = lr)\n",
    "loss_criteria = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training of VQVAE starts here...\")\n",
    "\n",
    "vqvae.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # overall_loss = 0\n",
    "    \n",
    "    for batch_idx, (x, _) in enumerate(train_dataloader):\n",
    "        x = x.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_cap, codebook_loss, commitment_loss, perplexity = vqvae(x)\n",
    "        \n",
    "        reconstruction_loss = loss_criteria(x_cap, x)\n",
    "        total_loss = reconstruction_loss + codebook_loss + commitment_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % print_step == 0:\n",
    "            print(f\"Epoch {epoch + 1}\\t batch_idx {batch_idx + 1}\\t reconstruction_loss = {reconstruction_loss}\\t codebook_loss = {codebook_loss} \\t commitment_loss = {commitment_loss}\\n, perplexity = {perplexity} \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample_image(x, postfix):\n",
    "      \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Visualization of {}\".format(postfix))\n",
    "    plt.imshow(np.transpose(make_grid(x.detach().cpu(), padding=2, normalize=True), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(tqdm(val_dataloader)):\n",
    "\n",
    "        x = x.to(DEVICE)\n",
    "        x_cap, codebook_loss, commitment_loss, perplexity = vqvae(x)\n",
    " \n",
    "        print(\"perplexity: \", perplexity.item(),\"commit_loss: \", commitment_loss.item(), \"  codebook loss: \", codebook_loss.item())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_sample_image(x[:BATCH_SIZE//2], \"Ground-truth images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_sample_image(x_cap[:BATCH_SIZE//2], \"Reconstructed images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AADARSH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
